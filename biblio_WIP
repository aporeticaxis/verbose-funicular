# verbose-funicular



@book{morgan_counterfactuals_2015,
	address = {New York, NY},
	edition = {Second Edition},
	series = {Analytical methods for social research},
	title = {Counterfactuals and causal inference: methods and principles for social research},
	isbn = {9781107065079 9781107694163},
	shorttitle = {Counterfactuals and causal inference},
	abstract = {"In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed"--},
	publisher = {Cambridge University Press},
	author = {Morgan, Stephen L. and Winship, Christopher},
	year = {2015},
	keywords = {Social sciences, Research, Social sciences, Methodology, Causation, MATHEMATICS / Probability \& Statistics / General},
}

@misc{christiano_finding_2022,
	title = {Finding gliders in the game of life},
	url = {https://ai-alignment.com/finding-gliders-in-the-game-of-life-b7c93b51079d},
	abstract = {Walking through a simple concrete example of ARC’s approach to ELK based on mechanistic anomaly detection.},
	language = {en},
	author = {Christiano, Paul},
	month = dec,
	year = {2022},
}

@misc{christiano_paul_notitle_2022,
	url = {https://www.lesswrong.com/posts/JLyWP2Y9LAruR2gi9/can-we-efficiently-distinguish-different-mechanisms},
	abstract = {(This post is an elaboration on “tractability of discrimination” as introduced in section III of Can we efficiently explain model behaviors? For an o…},
	language = {en},
	journal = {"Can we efficiently distinguish different mechanisms?"},
	author = {{Christiano, Paul}},
	month = dec,
	year = {2022},
}

@book{pearl_book_2018,
	address = {New York},
	edition = {First edition},
	title = {The book of why: the new science of cause and effect},
	isbn = {9780465097609},
	shorttitle = {The book of why},
	abstract = {"Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world"--},
	publisher = {Basic Books},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
	keywords = {Causation, Inference},
}

@misc{baez_what_2024,
	title = {What is {Entropy}?},
	url = {http://arxiv.org/abs/2409.09232},
	doi = {10.48550/arXiv.2409.09232},
	abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Baez, John C.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09232 [cond-mat, physics:math-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Mathematical Physics},
}

@misc{siegel_core-bench:_2024,
	title = {{CORE}-{Bench}: {Fostering} the {Credibility} of {Published} {Research} {Through} a {Computational} {Reproducibility} {Agent} {Benchmark}},
	shorttitle = {{CORE}-{Bench}},
	url = {http://arxiv.org/abs/2409.11363},
	doi = {10.48550/arXiv.2409.11363},
	abstract = {AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21\% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Siegel, Zachary S. and Kapoor, Sayash and Nagdir, Nitya and Stroebl, Benedikt and Narayanan, Arvind},
	month = sep,
	year = {2024},
	note = {arXiv:2409.11363 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{nemecek_coinductive_2023,
	title = {Coinductive guide to inductive transformer heads},
	url = {http://arxiv.org/abs/2302.01834},
	doi = {10.48550/arXiv.2302.01834},
	abstract = {We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra. Transformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse. Attention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras "tensors with a built-in loss function gradient". This loss function gradient occurs within the single layers and no backward pass is needed. This is in contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Nemecek, Adam},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01834 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{liu_retrievalattention:_2024,
	title = {{RetrievalAttention}: {Accelerating} {Long}-{Context} {LLM} {Inference} via {Vector} {Retrieval}},
	shorttitle = {{RetrievalAttention}},
	url = {http://arxiv.org/abs/2409.10516},
	doi = {10.48550/arXiv.2409.10516},
	abstract = {Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference latency and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to use approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieves the most relevant ones with vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation shows that RetrievalAttention only needs to access 1--3\% of data while maintaining high model accuracy. This leads to significant reduction in the inference cost of long-context LLMs with much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10516 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{imgur_attention_nodate,
	title = {Attention mechanism of the {Transformer}},
	url = {https://imgur.com/gallery/vuw15aL},
	abstract = {Discover topics like transformer, attention, and the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users like JaviiiiAbellan.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Imgur},
	author = {{Imgur}},
}

@misc{imgur_multihead_nodate,
	title = {{MultiHead} {Attention}},
	url = {https://imgur.com/gallery/FBQqrxw},
	abstract = {Discover topics like multihead, transformer, attention, and the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users like JaviiiiAbellan.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Imgur},
	author = {{Imgur}},
}

@misc{li_chain_2024,
	title = {Chain of {Thought} {Empowers} {Transformers} to {Solve} {Inherently} {Serial} {Problems}},
	url = {http://arxiv.org/abs/2402.12875},
	doi = {10.48550/arXiv.2402.12875},
	abstract = {Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length \$n\$, previous works have shown that constant-depth transformers with finite precision \${\textbackslash}mathsf\{poly\}(n)\$ embedding size can only solve problems in \${\textbackslash}mathsf\{TC\}{\textasciicircum}0\$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in \${\textbackslash}mathsf\{AC\}{\textasciicircum}0\$, a proper subset of \$ {\textbackslash}mathsf\{TC\}{\textasciicircum}0\$. However, with \$T\$ steps of CoT, constant-depth transformers using constant-bit precision and \$O({\textbackslash}log n)\$ embedding size can solve any problem solvable by boolean circuits of size \$T\$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
	month = may,
	year = {2024},
	note = {arXiv:2402.12875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity, Statistics - Machine Learning},
}

@misc{park_geometry_2024,
	title = {The {Geometry} of {Categorical} and {Hierarchical} {Concepts} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.01506},
	doi = {10.48550/arXiv.2406.01506},
	abstract = {Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as \{'mammal', 'bird', 'reptile', 'fish'\}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01506 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{scholze_lectures_2019,
	title = {Lectures on {Analytic} {Geometry}},
	url = {https://www.math.uni-bonn.de/people/scholze/Analytic.pdf},
	author = {Scholze, Peter},
	month = oct,
	year = {2019},
}

@misc{wu_sgformer:_2024,
	title = {{SGFormer}: {Single}-{Layer} {Graph} {Transformers} with {Approximation}-{Free} {Linear} {Complexity}},
	shorttitle = {{SGFormer}},
	url = {http://arxiv.org/abs/2409.09007},
	doi = {10.48550/arXiv.2409.09007},
	abstract = {Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wu, Qitian and Yang, Kai and Zhang, Hengrui and Wipf, David and Yan, Junchi},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09007 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{ashktorab_emerging_2024,
	title = {Emerging {Reliance} {Behaviors} in {Human}-{AI} {Text} {Generation}: {Hallucinations}, {Data} {Quality} {Assessment}, and {Cognitive} {Forcing} {Functions}},
	shorttitle = {Emerging {Reliance} {Behaviors} in {Human}-{AI} {Text} {Generation}},
	url = {http://arxiv.org/abs/2409.08937},
	doi = {10.48550/arXiv.2409.08937},
	abstract = {In this paper, we investigate the impact of hallucinations and cognitive forcing functions in human-AI collaborative text generation tasks, focusing on the use of Large Language Models (LLMs) to assist in generating high-quality conversational data. LLMs require data for fine-tuning, a crucial step in enhancing their performance. In the context of conversational customer support, the data takes the form of a conversation between a human customer and an agent and can be generated with an AI assistant. In our inquiry, involving 11 users who each completed 8 tasks, resulting in a total of 88 tasks, we found that the presence of hallucinations negatively impacts the quality of data. We also find that, although the cognitive forcing function does not always mitigate the detrimental effects of hallucinations on data quality, the presence of cognitive forcing functions and hallucinations together impacts data quality and influences how users leverage the AI responses presented to them. Our analysis of user behavior reveals distinct patterns of reliance on AI-generated responses, highlighting the importance of managing hallucinations in AI-generated content within conversational AI contexts.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ashktorab, Zahra and Pan, Qian and Geyer, Werner and Desmond, Michael and Danilevsky, Marina and Johnson, James M. and Dugan, Casey and Bachman, Michelle},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08937 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{van_rooij_reclaiming_2023,
	title = {Reclaiming {AI} as a theoretical tool for cognitive science},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://osf.io/4cbuv},
	doi = {10.31234/osf.io/4cbuv},
	abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
	urldate = {2024-09-19},
	author = {Van Rooij, Iris and Guest, Olivia and Adolfi, Federico G and De Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
	month = aug,
	year = {2023},
}

@misc{bowman_checklist:_2024,
	title = {The {Checklist}: {What} {Succeeding} at {AI} {Safety} {Will} {Involve}},
	shorttitle = {The {Checklist}},
	url = {https://sleepinyourhat.github.io/checklist/},
	abstract = {Sep 3, 2024 Preface This piece reflects my current best guess at the major goals that Anthropic (or another similarly positioned AI developer) will need to …},
	language = {en},
	author = {Bowman, Sam},
	month = sep,
	year = {2024},
}

@misc{brown_large_2024,
	title = {Large {Language} {Monkeys}: {Scaling} {Inference} {Compute} with {Repeated} {Sampling}},
	shorttitle = {Large {Language} {Monkeys}},
	url = {http://arxiv.org/abs/2407.21787},
	doi = {10.48550/arXiv.2407.21787},
	abstract = {Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9\% with one sample to 56\% with 250 samples, outperforming the single-attempt state-of-the-art of 43\% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95\% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and Ré, Christopher and Mirhoseini, Azalia},
	month = sep,
	year = {2024},
	note = {arXiv:2407.21787 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{sevilla_training_2024,
	title = {Training {Compute} of {Frontier} {AI} {Models} {Grows} by 4-5x per {Year}},
	url = {https://epochai.org/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year},
	abstract = {Our expanded AI model database shows that the compute used to train recent models grew 4-5x yearly from 2010 to May 2024. We find similar growth in frontier models, recent large language models, and models from leading companies.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Sevilla, Jaime},
	month = may,
	year = {2024},
}

@misc{cottier_how_2024,
	title = {How {Much} {Does} {It} {Cost} to {Train} {Frontier} {AI} {Models}?},
	url = {https://epochai.org/blog/how-much-does-it-cost-to-train-frontier-ai-models},
	abstract = {The cost of training frontier AI models has grown by a factor of 2 to 3x per year for the past eight years, suggesting that the largest models will cost over a billion dollars by 2027.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Cottier, Ben},
	month = jun,
	year = {2024},
}

@misc{sevilla_can_2024,
	title = {Can {AI} {Scaling} {Continue} {Through} 2030?},
	url = {https://epochai.org/blog/can-ai-scaling-continue-through-2030},
	abstract = {We investigate the scalability of AI training runs. We identify electric power, chip manufacturing, data and latency as constraints. We conclude that 2e29 FLOP training runs will likely be feasible by 2030.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Sevilla, Jaime},
	month = aug,
	year = {2024},
}

@misc{villalobos_trading_2023,
	title = {Trading {Off} {Compute} in {Training} and {Inference}},
	url = {https://epochai.org/blog/trading-off-compute-in-training-and-inference},
	abstract = {We explore several techniques that induce a tradeoff between spending more resources on training or on inference and characterize the properties of this tradeoff. We outline some implications for AI governance.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Villalobos, Pablo},
	month = jul,
	year = {2023},
}

@misc{lightman_lets_2023,
	title = {Let's {Verify} {Step} by {Step}},
	url = {http://arxiv.org/abs/2305.20050},
	doi = {10.48550/arXiv.2305.20050},
	abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	month = may,
	year = {2023},
	note = {arXiv:2305.20050 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{pfau_lets_2024,
	title = {Let's {Think} {Dot} by {Dot}: {Hidden} {Computation} in {Transformer} {Language} {Models}},
	shorttitle = {Let's {Think} {Dot} by {Dot}},
	url = {http://arxiv.org/abs/2404.15758},
	doi = {10.48550/arXiv.2404.15758},
	abstract = {Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15758 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.6},
}

@misc{prystawski_why_2023,
	title = {Why think step by step? {Reasoning} emerges from the locality of experience},
	shorttitle = {Why think step by step?},
	url = {http://arxiv.org/abs/2304.03843},
	doi = {10.48550/arXiv.2304.03843},
	abstract = {Humans have a powerful and mysterious capacity to reason. Working through a set of mental steps enables us to make inferences we would not be capable of making directly even though we get no additional data from the world. Similarly, when large language models generate intermediate steps (a chain of thought) before answering a question, they often produce better answers than they would directly. We investigate why and how chain-of-thought reasoning is useful in language models, testing the hypothesis that reasoning is effective when training data consists of overlapping local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences to estimate relationships between variables that were not seen together in training. We prove that there will exist a "reasoning gap", where reasoning through intermediate variables reduces bias, for the simple case of an autoregressive density estimator trained on local samples from a chain-structured probabilistic model. We then test our hypothesis experimentally in more complex models, training an autoregressive language model on samples from Bayes nets but only including a subset of variables in each sample. We test language models' ability to match conditional probabilities with and without intermediate reasoning steps, finding that intermediate steps are only helpful when the training data is locally structured with respect to dependencies between variables. The combination of locally structured observations and reasoning is much more data-efficient than training on all variables. Our results illustrate how the effectiveness of reasoning step by step is rooted in the local statistical structure of the training data.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Prystawski, Ben and Li, Michael Y. and Goodman, Noah D.},
	month = nov,
	year = {2023},
	note = {arXiv:2304.03843 [cs]
version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{milicka_large_2024,
	title = {Large language models are able to downplay their cognitive abilities to fit the persona they simulate},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0298522},
	doi = {10.1371/journal.pone.0298522},
	abstract = {This study explores the capabilities of large language models to replicate the behavior of individuals with underdeveloped cognitive and language skills. Specifically, we investigate whether these models can simulate child-like language and cognitive development while solving false-belief tasks, namely, change-of-location and unexpected-content tasks. GPT-3.5-turbo and GPT-4 models by OpenAI were prompted to simulate children (N = 1296) aged one to six years. This simulation was instantiated through three types of prompts: plain zero-shot, chain-of-thoughts, and primed-by-corpus. We evaluated the correctness of responses to assess the models’ capacity to mimic the cognitive skills of the simulated children. Both models displayed a pattern of increasing correctness in their responses and rising language complexity. That is in correspondence with a gradual enhancement in linguistic and cognitive abilities during child development, which is described in the vast body of research literature on child development. GPT-4 generally exhibited a closer alignment with the developmental curve observed in ‘real’ children. However, it displayed hyper-accuracy under certain conditions, notably in the primed-by-corpus prompt type. Task type, prompt type, and the choice of language model influenced developmental patterns, while temperature and the gender of the simulated parent and child did not consistently impact results. We conducted analyses of linguistic complexity, examining utterance length and Kolmogorov complexity. These analyses revealed a gradual increase in linguistic complexity corresponding to the age of the simulated children, regardless of other variables. These findings show that the language models are capable of downplaying their abilities to achieve a faithful simulation of prompted personas.},
	language = {en},
	number = {3},
	urldate = {2024-09-19},
	journal = {PLOS ONE},
	author = {Milička, Jiří and Marklová, Anna and VanSlambrouck, Klára and Pospíšilová, Eva and Šimsová, Jana and Harvan, Samuel and Drobil, Ondřej},
	month = mar,
	year = {2024},
	keywords = {Language, Theory of mind, Charts, Chocolate, Cognitive linguistics, Children, Kolmogorov complexity, Psycholinguistics},
	pages = {e0298522},
}

@misc{tian_macgyver:_2024,
	title = {{MacGyver}: {Are} {Large} {Language} {Models} {Creative} {Problem} {Solvers}?},
	shorttitle = {{MacGyver}},
	url = {http://arxiv.org/abs/2311.09682},
	doi = {10.48550/arXiv.2311.09682},
	abstract = {We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Tian, Yufei and Ravichander, Abhilasha and Qin, Lianhui and Bras, Ronan Le and Marjieh, Raja and Peng, Nanyun and Choi, Yejin and Griffiths, Thomas L. and Brahman, Faeze},
	month = mar,
	year = {2024},
	note = {arXiv:2311.09682 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{zhou_self-discover:_2024,
	title = {Self-{Discover}: {Large} {Language} {Models} {Self}-{Compose} {Reasoning} {Structures}},
	shorttitle = {Self-{Discover}},
	url = {https://arxiv.org/abs/2402.03620v1},
	abstract = {We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32\% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20\%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
	month = feb,
	year = {2024},
}

@article{kambhampati_can_2024,
	title = {Can {Large} {Language} {Models} {Reason} and {Plan}?},
	volume = {1534},
	issn = {0077-8923, 1749-6632},
	url = {http://arxiv.org/abs/2403.04121},
	doi = {10.1111/nyas.15125},
	abstract = {While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
	number = {1},
	urldate = {2024-09-19},
	journal = {Annals of the New York Academy of Sciences},
	author = {Kambhampati, Subbarao},
	month = apr,
	year = {2024},
	note = {arXiv:2403.04121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {15--18},
}

@misc{carrasco-farre_large_2024,
	title = {Large {Language} {Models} are as persuasive as humans, but how? {About} the cognitive effort and moral-emotional language of {LLM} arguments},
	shorttitle = {Large {Language} {Models} are as persuasive as humans, but how?},
	url = {http://arxiv.org/abs/2404.09329},
	doi = {10.48550/arXiv.2404.09329},
	abstract = {Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Carrasco-Farre, Carlos},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09329 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jiang_peek_2024,
	title = {A {Peek} into {Token} {Bias}: {Large} {Language} {Models} {Are} {Not} {Yet} {Genuine} {Reasoners}},
	shorttitle = {A {Peek} into {Token} {Bias}},
	url = {https://arxiv.org/abs/2406.11050v1},
	abstract = {This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Jiang, Bowen and Xie, Yangxinyu and Hao, Zhuoqun and Wang, Xiaomeng and Mallick, Tanwi and Su, Weijie J. and Taylor, Camillo J. and Roth, Dan},
	month = jun,
	year = {2024},
}

@misc{macmillan-scott_irrationality_2024,
	title = {({Ir})rationality and {Cognitive} {Biases} in {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2402.09193v2},
	abstract = {Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Macmillan-Scott, Olivia and Musolesi, Mirco},
	month = feb,
	year = {2024},
}

@misc{noauthor_self-discover/self_discover.py_nodate,
	title = {self-discover/self\_discover.py at main · catid/self-discover},
	url = {https://github.com/catid/self-discover/blob/main/self_discover.py},
	abstract = {Implementation of Google's SELF-DISCOVER. Contribute to catid/self-discover development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-09-19},
	journal = {GitHub},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {https://arxiv.org/abs/2305.10601v2},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = may,
	year = {2023},
}

@misc{noauthor_ideonomy_nodate,
	title = {Ideonomy},
	url = {https://ideonomy.mit.edu/},
	urldate = {2024-09-19},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	doi = {10.48550/arXiv.2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
}

@misc{burger_truth_2024,
	title = {Truth is {Universal}: {Robust} {Detection} of {Lies} in {LLMs}},
	shorttitle = {Truth is {Universal}},
	url = {http://arxiv.org/abs/2407.12831},
	doi = {10.48550/arXiv.2407.12831},
	abstract = {Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of "lying", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, distinguishing simple true and false statements with 94\% accuracy and detecting more complex real-world lies with 95\% accuracy.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Bürger, Lennart and Hamprecht, Fred A. and Nadler, Boaz},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12831 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{bharti_chartom:_2024,
	title = {{CHARTOM}: {A} {Visual} {Theory}-of-{Mind} {Benchmark} for {Multimodal} {Large} {Language} {Models}},
	shorttitle = {{CHARTOM}},
	url = {http://arxiv.org/abs/2408.14419},
	doi = {10.48550/arXiv.2408.14419},
	abstract = {We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Bharti, Shubham and Cheng, Shiyun and Rho, Jihyun and Rao, Martina and Zhu, Xiaojin},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14419 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gurnee_finding_2023,
	title = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
	shorttitle = {Finding {Neurons} in a {Haystack}},
	url = {http://arxiv.org/abs/2305.01610},
	doi = {10.48550/arXiv.2305.01610},
	abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
	month = jun,
	year = {2023},
	note = {arXiv:2305.01610 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{bricken_towards_2023,
	title = {"{Towards} {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}",},
	journal = {Transformer Circuits Thread},
	author = {Bricken, , et al.},
	year = {2023},
}

@misc{templeton_et_al._scaling_2024,
	title = {Scaling {Monosemanticity}: {Extracting} {Interpretable} {Features} from {Claude} 3 {Sonnet}},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/},
	journal = {Transformer Circuits Thread},
	author = {Templeton et al.,},
	month = may,
	year = {2024},
}

@misc{huben_comments_2024,
	type = {Substack newsletter},
	title = {Comments on {Anthropic}'s {Scaling} {Monosemanticity}},
	url = {https://aizi.substack.com/p/comments-on-anthropics-scaling-monosemanticity},
	abstract = {Anthropic recently released a research report on sparse autoencoders, Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. These are some thoughts on it. (This is a very technical and inside-baseball post, so it may not be especially interesting to every reader.)},
	urldate = {2024-09-19},
	journal = {From AI to ZI},
	author = {Huben, Robert},
	month = jun,
	year = {2024},
}

@article{johnswentworth_whats_2022,
	title = {What's {General}-{Purpose} {Search}, {And} {Why} {Might} {We} {Expect} {To} {See} {It} {In} {Trained} {ML} {Systems}?},
	url = {https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see},
	abstract = {Benito has an interesting job. Here’s some of the stuff he’s had to do over the past couple years: …},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = aug,
	year = {2022},
}

@article{johnswentworth_you_2024,
	title = {You {Are} {Not} {Measuring} {What} {You} {Think} {You} {Are} {Measuring}},
	url = {https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring},
	abstract = {Two laws of experiment design: First, you are not measuring what you think you are measuring. Second, if you measure enough different stuff, you migh…},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = sep,
	year = {2024},
}

@misc{lubana_mechanistic_2023,
	title = {Mechanistic {Mode} {Connectivity}},
	url = {http://arxiv.org/abs/2211.08422},
	doi = {10.48550/arXiv.2211.08422},
	abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model's reliance on spurious attributes.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lubana, Ekdeep Singh and Bigelow, Eric J. and Dick, Robert P. and Krueger, David and Tanaka, Hidenori},
	month = jun,
	year = {2023},
	note = {arXiv:2211.08422 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{johnswentworth_how_2024,
	title = {How {To} {Go} {From} {Interpretability} {To} {Alignment}: {Just} {Retarget} {The} {Search}},
	shorttitle = {How {To} {Go} {From} {Interpretability} {To} {Alignment}},
	url = {https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget},
	abstract = {Here's a simple strategy for AI alignment: use interpretability tools to identify the AI's internal search process, and the AI's internal representat…},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = sep,
	year = {2024},
}

@article{paape_how_2024,
	title = {How do linguistic illusions arise? {Rational} inference and good-enough processing as competing latent processes within individuals},
	issn = {2327-3801},
	shorttitle = {How do linguistic illusions arise?},
	doi = {10.1080/23273798.2024.2387226},
	abstract = {Non-literal interpretations of implausible sentences such as The mother gave the candle the daughter have been taken as evidence for a rational error-correction mechanism that reconstructs the intended utterance from the ill-formed input (…gave the daughter the candle). However, the good-enough processing framework offers an alternative explanation: readers sometimes miss problematic aspects of sentences because they are only processing them superficially, which leads to acceptability illusions. As a synthesis of these accounts, I propose that conscious rational inferences about errors on the one hand and good-enough processing on the other are competing latent processes that simultaneously occur within the same comprehender. In support of this view, I present data from a two-dimensional grammaticality/interpretability judgment task with different types of subtly ill-formed sentences. Both conscious rational inference and good-enough processing predict positive interpretability judgments for such sentences, but only good-enough processing also predicts positive grammaticality judgments. By fitting a lognormal race model jointly to judgments and response latencies, I show that conscious rational inference and good-enough processing, as well as purely grammar-driven processing, actively trade off with each other during reading. Furthermore, individual differences measures reveal that participant traits such as linguistic pedantry, interpretational charity, and analytic/intuitive cognitive styles contribute to variability in the processing patterns. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
	journal = {Language, Cognition and Neuroscience},
	author = {Paape, Dario},
	month = may,
	year = {2024},
	keywords = {Cognitive Processes, Grammar, Individual Differences, Judgment, Linguistics, Reading Comprehension},
}

@misc{noauthor_whats_2024,
	title = {What’s in a \$\&!\#* vector?},
	url = {https://www.juriopitz.com/2024/04/04/explain-text-similarity.html},
	abstract = {What’s in a \$\&!\#* vector? Explaining text embeddings and text similarity},
	language = {en-US},
	urldate = {2024-09-19},
	journal = {Juri Opitz},
	month = apr,
	year = {2024},
}

@inproceedings{opitz_sbert_2022,
	address = {Online only},
	title = {{SBERT} studies {Meaning} {Representations}: {Decomposing} {Sentence} {Embeddings} into {Explainable} {Semantic} {Features}},
	shorttitle = {{SBERT} studies {Meaning} {Representations}},
	url = {https://aclanthology.org/2022.aacl-main.48},
	abstract = {Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce Semantically Structured Sentence BERT embeddings (S{\textasciicircum}3BERT). Our S{\textasciicircum}3BERT embeddings are composed of explainable sub-embeddings that emphasize various sentence meaning features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into meaning features, through approximation of a suite of interpretable semantic AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability – while preserving the effectiveness and efficiency of the neural sentence embeddings.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Opitz, Juri and Frank, Anette},
	editor = {He, Yulan and Ji, Heng and Li, Sujian and Liu, Yang and Chang, Chua-Hui},
	month = nov,
	year = {2022},
	pages = {625--638},
}

@inproceedings{moeller_attribution_2023,
	address = {Singapore},
	title = {An {Attribution} {Method} for {Siamese} {Encoders}},
	url = {https://aclanthology.org/2023.emnlp-main.980},
	doi = {10.18653/v1/2023.emnlp-main.980},
	abstract = {Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The output takes the form of feature-pair attributions and in case of STs it can be reduced to a token–token matrix. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in case of STs few token pairs can dominate predictions and that STs preferentially focus on nouns and verbs. For accurate predictions, however, they need to attend to the majority of tokens and parts of speech.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Moeller, Lucas and Nikolaev, Dmitry and Padó, Sebastian},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15818--15827},
}

@misc{nastase_grammatical_2023,
	title = {Grammatical information in {BERT} sentence embeddings as two-dimensional arrays},
	url = {http://arxiv.org/abs/2312.09890},
	doi = {10.48550/arXiv.2312.09890},
	abstract = {Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Nastase, Vivi and Merlo, Paola},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09890 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{durvasula_distilling_2024,
	title = {Distilling {Data} from {Large} {Language} {Models}: {An} {Application} to {Research} {Productivity} {Measurement}},
	shorttitle = {Distilling {Data} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.08030},
	doi = {10.48550/arXiv.2405.08030},
	abstract = {We develop a method for assigning high-quality labels to unstructured text. The method is based on fine-tuning an open-source language model with data extracted from a proprietary large language model. We apply this method to construct a census of published clinical trials. We revisit a literature that contends that pharmaceutical research productivity is declining, based on measured increases in the quantity of clinical trials, which outpace trends in output. In our data, the quantity and composition of trials are stable since 2010. Previous measurements are an artifact of biases driven by shifts in the composition of other forms of research.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Durvasula, Maya M. and Eyuboglu, Sabri and Ritzwoller, David M.},
	month = sep,
	year = {2024},
	note = {arXiv:2405.08030 [econ, q-fin]},
	keywords = {Economics - General Economics},
}

@misc{maas_complex_2024,
	title = {Complex {Systems} {Research} in {Psychology}},
	url = {https://santafeinstitute.github.io/ComplexPsych/},
	language = {en},
	urldate = {2024-09-19},
	author = {Maas, Han L. J. van der},
	month = sep,
	year = {2024},
}

@misc{ma_unifying_2024,
	title = {Unifying {Multimodal} {Retrieval} via {Document} {Screenshot} {Embedding}},
	url = {http://arxiv.org/abs/2406.11251},
	doi = {10.48550/arXiv.2406.11251},
	abstract = {In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding\vphantom{\{}\} (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ma, Xueguang and Lin, Sheng-Chieh and Li, Minghan and Chen, Wenhu and Lin, Jimmy},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11251 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{sutherland_are_2024,
	title = {Are {We} {Now} {Too} {Impatient} to {Be} {Intelligent}? - {YouTube}},
	url = {https://www.youtube.com/watch?v=Bc9jFbxrkMk},
	author = {Sutherland, Rory},
	month = jul,
	year = {2024},
	note = {publisher: Nudgestock},
}

@misc{chang_how_2024,
	title = {How {Do} {Large} {Language} {Models} {Acquire} {Factual} {Knowledge} {During} {Pretraining}?},
	url = {http://arxiv.org/abs/2406.11813},
	doi = {10.48550/arXiv.2406.11813},
	abstract = {Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Chang, Hoyeon and Park, Jinho and Ye, Seonghyeon and Yang, Sohee and Seo, Youngkyung and Chang, Du-Seong and Seo, Minjoon},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11813 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{elhoushi_layerskip:_2024,
	title = {{LayerSkip}: {Enabling} {Early} {Exit} {Inference} and {Self}-{Speculative} {Decoding}},
	shorttitle = {{LayerSkip}},
	url = {http://arxiv.org/abs/2404.16710},
	doi = {10.48550/arXiv.2404.16710},
	abstract = {We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and Aly, Ahmed A. and Chen, Beidi and Wu, Carole-Jean},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16710 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_grokked_2024,
	title = {Grokked {Transformers} are {Implicit} {Reasoners}: {A} {Mechanistic} {Journey} to the {Edge} of {Generalization}},
	shorttitle = {Grokked {Transformers} are {Implicit} {Reasoners}},
	url = {http://arxiv.org/abs/2405.15071},
	doi = {10.48550/arXiv.2405.15071},
	abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
	month = may,
	year = {2024},
	note = {arXiv:2405.15071 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_premise_2024,
	title = {Premise {Order} {Matters} in {Reasoning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.08939},
	doi = {10.48550/arXiv.2402.08939},
	abstract = {Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30\%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Chen, Xinyun and Chi, Ryan A. and Wang, Xuezhi and Zhou, Denny},
	month = may,
	year = {2024},
	note = {arXiv:2402.08939 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{smith_end_2024,
	title = {The end of accountability},
	url = {https://www.newstatesman.com/culture/books/book-of-the-day/2024/07/the-end-of-accountability},
	abstract = {In politics and business, faceless systems have taken over decision-making and infantilised socity.},
	language = {en-US},
	urldate = {2024-09-19},
	journal = {New Statesman},
	author = {Smith, Ed},
	month = jul,
	year = {2024},
}

@misc{gichamba_colbert_2024,
	title = {{ColBERT} {Retrieval} and {Ensemble} {Response} {Scoring} for {Language} {Model} {Question} {Answering}},
	url = {http://arxiv.org/abs/2408.10808},
	doi = {10.48550/arXiv.2408.10808},
	abstract = {Domain-specific question answering remains challenging for language models, given the deep technical knowledge required to answer questions correctly. This difficulty is amplified for smaller language models that cannot encode as much information in their parameters as larger models. The "Specializing Large Language Models for Telecom Networks" challenge aimed to enhance the performance of two small language models, Phi-2 and Falcon-7B in telecommunication question answering. In this paper, we present our question answering systems for this challenge. Our solutions achieved leading marks of 81.9\% accuracy for Phi-2 and 57.3\% for Falcon-7B. We have publicly released our code and fine-tuned models.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Gichamba, Alex and Idris, Tewodros Kederalah and Ebiyau, Brian and Nyberg, Eric and Mitamura, Teruko},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10808 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{noauthor_scavenging_nodate,
	title = {Scavenging for cognitive toolkits - tis.so},
	url = {https://tis.so/scavenging-for-cognitive-toolkits},
	abstract = {by Frances Kafka Quite often it’s useful to come up with rough binaries as a quick and ready classification tool. There are two general approaches},
	urldate = {2024-09-19},
}

@misc{li_quantifying_2024,
	title = {Quantifying {AI} {Psychology}: {A} {Psychometrics} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {Quantifying {AI} {Psychology}},
	url = {http://arxiv.org/abs/2406.17675},
	doi = {10.48550/arXiv.2406.17675},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Li, Yuan and Huang, Yue and Wang, Hongyi and Zhang, Xiangliang and Zou, James and Sun, Lichao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17675 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{reser_cognitive_2023,
	title = {A {Cognitive} {Architecture} for {Machine} {Consciousness} and {Artificial} {Superintelligence}: {Thought} {Is} {Structured} by the {Iterative} {Updating} of {Working} {Memory}},
	shorttitle = {A {Cognitive} {Architecture} for {Machine} {Consciousness} and {Artificial} {Superintelligence}},
	url = {http://arxiv.org/abs/2203.17255},
	doi = {10.48550/arXiv.2203.17255},
	abstract = {This article provides an analytical framework for how to simulate human-like thought processes within a computer. It describes how attention and memory should be structured, updated, and utilized to search for associative additions to the stream of thought. The focus is on replicating the dynamics of the mammalian working memory system, which features two forms of persistent activity: sustained firing (preserving information on the order of seconds) and synaptic potentiation (preserving information from minutes to hours). The article uses a series of over 40 original figures to systematically demonstrate how the iterative updating of these working memory stores provides functional structure to behavior, cognition, and consciousness. In an AI implementation, these two memory stores should be updated continuously and in an iterative fashion, meaning each state should preserve a proportion of the coactive representations from the state before it. Thus, the set of concepts in working memory will evolve gradually and incrementally over time. This makes each state a revised iteration of the preceding state and causes successive states to overlap and blend with respect to the information they contain. Transitions between states happen as persistent activity spreads activation energy throughout the hierarchical network searching long-term memory for the most appropriate representation to be added to the global workspace. The result is a chain of associatively linked intermediate states capable of advancing toward a solution or goal. Iterative updating is conceptualized here as an information processing strategy, a model of working memory, a theory of consciousness, and an algorithm for designing and programming artificial general intelligence.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Reser, Jared Edward},
	month = dec,
	year = {2023},
	note = {arXiv:2203.17255 [cs, q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lu_ai_2024,
	title = {The {AI} {Scientist}: {Towards} {Fully} {Automated} {Open}-{Ended} {Scientific} {Discovery}},
	shorttitle = {The {AI} {Scientist}},
	url = {http://arxiv.org/abs/2408.06292},
	doi = {10.48550/arXiv.2408.06292},
	abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yao_react:_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zelikman_quiet-star:_2024,
	title = {Quiet-{STaR}: {Language} {Models} {Can} {Teach} {Themselves} to {Think} {Before} {Speaking}},
	shorttitle = {Quiet-{STaR}},
	url = {http://arxiv.org/abs/2403.09629},
	doi = {10.48550/arXiv.2403.09629},
	abstract = {When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9\%\${\textbackslash}rightarrow\$10.9\%) and CommonsenseQA (36.3\%\${\textbackslash}rightarrow\$47.2\%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_executable_2024,
	title = {Executable {Code} {Actions} {Elicit} {Better} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2402.01030},
	doi = {10.48550/arXiv.2402.01030},
	abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
	month = jun,
	year = {2024},
	note = {arXiv:2402.01030 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{didolkar_metacognitive_2024,
	title = {Metacognitive {Capabilities} of {LLMs}: {An} {Exploration} in {Mathematical} {Problem} {Solving}},
	shorttitle = {Metacognitive {Capabilities} of {LLMs}},
	url = {http://arxiv.org/abs/2405.12205},
	doi = {10.48550/arXiv.2405.12205},
	abstract = {Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Didolkar, Aniket and Goyal, Anirudh and Ke, Nan Rosemary and Guo, Siyuan and Valko, Michal and Lillicrap, Timothy and Rezende, Danilo and Bengio, Yoshua and Mozer, Michael and Arora, Sanjeev},
	month = may,
	year = {2024},
	note = {arXiv:2405.12205 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{besta_graph_2024,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	doi = {10.1609/aaai.v38i16.29720},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	number = {16},
	urldate = {2024-09-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = mar,
	year = {2024},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {17682--17690},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	doi = {10.48550/arXiv.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{wang_mixture--agents_2024,
	title = {Mixture-of-{Agents} {Enhances} {Large} {Language} {Model} {Capabilities}},
	url = {http://arxiv.org/abs/2406.04692},
	doi = {10.48550/arXiv.2406.04692},
	abstract = {Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1\% compared to 57.5\% by GPT-4 Omni.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{valmeekam_planbench:_2023,
	title = {{PlanBench}: {An} {Extensible} {Benchmark} for {Evaluating} {Large} {Language} {Models} on {Planning} and {Reasoning} about {Change}},
	shorttitle = {{PlanBench}},
	url = {http://arxiv.org/abs/2206.10498},
	doi = {10.48550/arXiv.2206.10498},
	abstract = {Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2206.10498 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{ou_counterfactual_2022,
	title = {Counterfactual {Data} {Augmentation} via {Perspective} {Transition} for {Open}-{Domain} {Dialogues}},
	url = {http://arxiv.org/abs/2210.16838},
	doi = {10.48550/arXiv.2210.16838},
	abstract = {The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is labor-intensive and time-consuming. In this paper, we propose a data augmentation method to automatically augment high-quality responses with different semantics by counterfactual inference. Specifically, given an observed dialogue, our counterfactual generation model first infers semantically different responses by replacing the observed reply perspective with substituted ones. Furthermore, our data selection method filters out detrimental augmented responses. Experimental results show that our data augmentation method can augment high-quality responses with different semantics for a given dialogue history, and can outperform competitive baselines on multiple downstream tasks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ou, Jiao and Zhang, Jinchao and Feng, Yang and Zhou, Jie},
	month = oct,
	year = {2022},
	note = {arXiv:2210.16838 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{levy_same_2024,
	title = {Same {Task}, {More} {Tokens}: the {Impact} of {Input} {Length} on the {Reasoning} {Performance} of {Large} {Language} {Models}},
	shorttitle = {Same {Task}, {More} {Tokens}},
	url = {http://arxiv.org/abs/2402.14848},
	doi = {10.48550/arXiv.2402.14848},
	abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
	month = jul,
	year = {2024},
	note = {arXiv:2402.14848 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{ball_extraordinary_2024,
	title = {An {Extraordinary} {Alien}},
	url = {https://www.hyperdimensional.co/p/an-extraordinary-alien},
	abstract = {OpenAI's o1 Challenges AI Policy},
	language = {en},
	author = {Ball, Dean W.},
	month = sep,
	year = {2024},
}

@misc{samadarshi_connecting_2024,
	title = {Connecting the {Dots}: {Evaluating} {Abstract} {Reasoning} {Capabilities} of {LLMs} {Using} the {New} {York} {Times} {Connections} {Word} {Game}},
	shorttitle = {Connecting the {Dots}},
	url = {http://arxiv.org/abs/2406.11012},
	doi = {10.48550/arXiv.2406.11012},
	abstract = {The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8\% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Samadarshi, Prisha and Mustafa, Mariam and Kulkarni, Anushka and Rothkopf, Raven and Chakrabarty, Tuhin and Muresan, Smaranda},
	month = jul,
	year = {2024},
	note = {arXiv:2406.11012 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@article{graeber_stories_2024,
	title = {Stories, {Statistics}, and {Memory}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0033-5533, 1531-4650},
	url = {https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae020/7691253},
	doi = {10.1093/qje/qjae020},
	abstract = {Abstract
            For many decisions, we encounter relevant information over the course of days, months, or years. We consume such information in various forms, including stories (qualitative content about individual instances) and statistics (quantitative data about collections of observations). This article proposes that information type—story versus statistic—shapes selective memory. In controlled experiments, we document a pronounced story-statistic gap in memory: the average impact of statistics on beliefs fades by 73\% over the course of a day, but the impact of a story fades by only 32\%. Guided by a model of selective memory, we disentangle different mechanisms and document that similarity relationships drive this gap. Recall of a story increases when its qualitative content is more similar to a memory prompt. Irrelevant information in memory that is similar to the prompt, on the other hand, competes for retrieval with relevant information, impeding successful recall.},
	language = {en},
	urldate = {2024-09-19},
	journal = {The Quarterly Journal of Economics},
	author = {Graeber, Thomas and Roth, Christopher and Zimmermann, Florian},
	month = jun,
	year = {2024},
	pages = {qjae020},
}

@misc{atreja_prompt_2024,
	title = {Prompt {Design} {Matters} for {Computational} {Social} {Science} {Tasks} but in {Unpredictable} {Ways}},
	url = {http://arxiv.org/abs/2406.11980},
	doi = {10.48550/arXiv.2406.11980},
	abstract = {Manually annotating data for computational social science tasks can be costly, time-consuming, and emotionally draining. While recent work suggests that LLMs can perform such annotation tasks in zero-shot settings, little is known about how prompt design impacts LLMs' compliance and accuracy. We conduct a large-scale multi-prompt experiment to test how model selection (ChatGPT, PaLM2, and Falcon7b) and prompt design features (definition inclusion, output type, explanation, and prompt length) impact the compliance and accuracy of LLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor stance, and news frames). Our results show that LLM compliance and accuracy are highly prompt-dependent. For instance, prompting for numerical scores instead of labels reduces all LLMs' compliance and accuracy. The overall best prompting setup is task-dependent, and minor prompt changes can cause large changes in the distribution of generated labels. By showing that prompt design significantly impacts the quality and distribution of LLM-generated annotations, this work serves as both a warning and practical guide for researchers and practitioners.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Atreja, Shubham and Ashkinaze, Joshua and Li, Lingyao and Mendelsohn, Julia and Hemphill, Libby},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11980 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{schulhoff_prompt_2024,
	title = {The {Prompt} {Report}: {A} {Systematic} {Survey} of {Prompting} {Techniques}},
	shorttitle = {The {Prompt} {Report}},
	url = {http://arxiv.org/abs/2406.06608},
	doi = {10.48550/arXiv.2406.06608},
	abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
	month = jul,
	year = {2024},
	note = {arXiv:2406.06608 [cs]
version: 3},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{liu_minds_2022,
	title = {Mind's {Eye}: {Grounded} {Language} {Model} {Reasoning} through {Simulation}},
	shorttitle = {Mind's {Eye}},
	url = {http://arxiv.org/abs/2210.05359},
	doi = {10.48550/arXiv.2210.05359},
	abstract = {Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05359 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{wu_minds_2024,
	title = {Mind's {Eye} of {LLMs}: {Visualization}-of-{Thought} {Elicits} {Spatial} {Reasoning} in {Large} {Language} {Models}},
	shorttitle = {Mind's {Eye} of {LLMs}},
	url = {http://arxiv.org/abs/2404.03622},
	doi = {10.48550/arXiv.2404.03622},
	abstract = {Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2404.03622 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huh_platonic_2024,
	title = {The {Platonic} {Representation} {Hypothesis}},
	url = {http://arxiv.org/abs/2405.07987},
	doi = {10.48550/arXiv.2405.07987},
	abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
	month = jul,
	year = {2024},
	note = {arXiv:2405.07987 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{goldstein_alignment_2024,
	title = {Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-46631-y},
	doi = {10.1038/s41467-024-46631-y},
	abstract = {Contextual embeddings, derived from deep language models (DLMs), provide a continuous vectorial representation of language. This embedding space differs fundamentally from the symbolic representations posited by traditional psycholinguistics. We hypothesize that language areas in the human brain, similar to DLMs, rely on a continuous embedding space to represent language. To test this hypothesis, we densely record the neural activity patterns in the inferior frontal gyrus (IFG) of three participants using dense intracranial arrays while they listened to a 30-minute podcast. From these fine-grained spatiotemporal neural recordings, we derive a continuous vectorial representation for each word (i.e., a brain embedding) in each patient. Using stringent zero-shot mapping we demonstrate that brain embeddings in the IFG and the DLM contextual embedding space have common geometric patterns. The common geometric patterns allow us to predict the brain embedding in IFG of a given left-out word based solely on its geometrical relationship to other non-overlapping words in the podcast. Furthermore, we show that contextual embeddings capture the geometry of IFG embeddings better than static word embeddings. The continuous brain embedding space exposes a vector-based neural code for natural language processing in the human brain.},
	language = {en},
	number = {1},
	urldate = {2024-09-19},
	journal = {Nature Communications},
	author = {Goldstein, Ariel and Grinstein-Dabush, Avigail and Schain, Mariano and Wang, Haocheng and Hong, Zhuoqiao and Aubrey, Bobbi and Schain, Mariano and Nastase, Samuel A. and Zada, Zaid and Ham, Eric and Feder, Amir and Gazula, Harshvardhan and Buchnik, Eliav and Doyle, Werner and Devore, Sasha and Dugan, Patricia and Reichart, Roi and Friedman, Daniel and Brenner, Michael and Hassidim, Avinatan and Devinsky, Orrin and Flinker, Adeen and Hasson, Uri},
	month = mar,
	year = {2024},
	keywords = {Language, Neural decoding, Neural encoding},
	pages = {2768},
}

@article{romera-paredes_mathematical_2023,
	title = {Mathematical discoveries from program search with large language models},
	volume = {625},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	shorttitle = {{FunSearch}},
	url = {https://www.nature.com/articles/s41586-023-06924-6},
	doi = {10.1038/s41586-023-06924-6},
	language = {en},
	number = {7995},
	journal = {Nature},
	author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
	month = dec,
	year = {2023},
	keywords = {Computer science, Pure mathematics},
	pages = {468--475},
}

@misc{staab_beyond_2024,
	title = {Beyond {Memorization}: {Violating} {Privacy} {Via} {Inference} with {Large} {Language} {Models}},
	shorttitle = {Beyond {Memorization}},
	url = {http://arxiv.org/abs/2310.07298},
	doi = {10.48550/arXiv.2310.07298},
	abstract = {Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to \$85{\textbackslash}\%\$ top-1 and \$95{\textbackslash}\%\$ top-3 accuracy at a fraction of the cost (\$100{\textbackslash}times\$) and time (\$240{\textbackslash}times\$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Staab, Robin and Vero, Mark and Balunović, Mislav and Vechev, Martin},
	month = may,
	year = {2024},
	note = {arXiv:2310.07298 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.7},
}

@misc{geiping_coercing_2024,
	title = {Coercing {LLMs} to do and reveal (almost) anything},
	url = {http://arxiv.org/abs/2402.14020},
	doi = {10.48550/arXiv.2402.14020},
	abstract = {It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{holst_dataset_2024,
	title = {Dataset {Artefacts} are the {Hidden} {Drivers} of the {Declining} {Disruptiveness} in {Science}},
	url = {http://arxiv.org/abs/2402.14583},
	doi = {10.48550/arXiv.2402.14583},
	abstract = {Park et al. [1] reported a decline in the disruptiveness of scientific and technological knowledge over time. Their main finding is based on the computation of CD indices, a measure of disruption in citation networks [2], across almost 45 million papers and 3.9 million patents. Due to a factual plotting mistake, database entries with zero references were omitted in the CD index distributions, hiding a large number of outliers with a maximum CD index of one, while keeping them in the analysis [1]. Our reanalysis shows that the reported decline in disruptiveness can be attributed to a relative decline of these database entries with zero references. Notably, this was not caught by the robustness checks included in the manuscript. The regression adjustment fails to control for the hidden outliers as they correspond to a discontinuity in the CD index. Proper evaluation of the Monte-Carlo simulations reveals that, because of the preservation of the hidden outliers, even random citation behaviour replicates the observed decline in disruptiveness. Finally, while these papers and patents with supposedly zero references are the hidden drivers of the reported decline, their source documents predominantly do make references, exposing them as pure dataset artefacts.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Holst, Vincent and Algaba, Andres and Tori, Floriano and Wenmackers, Sylvia and Ginis, Vincent},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14583 [cs]},
	keywords = {Computer Science - Digital Libraries, Computer Science - Social and Information Networks},
}

@misc{boiko_emergent_2023,
	title = {Emergent autonomous scientific research capabilities of large language models},
	url = {http://arxiv.org/abs/2304.05332},
	doi = {10.48550/arXiv.2304.05332},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05332 [physics]},
	keywords = {Physics - Chemical Physics, Computer Science - Computation and Language},
}

@article{spivak_ologs:_2012,
	title = {Ologs: a categorical framework for knowledge representation},
	volume = {7},
	issn = {1932-6203},
	shorttitle = {Ologs},
	url = {http://arxiv.org/abs/1102.1889},
	doi = {10.1371/journal.pone.0024274},
	abstract = {In this paper we introduce the olog, or ontology log, a category-theoretic model for knowledge representation (KR). Grounded in formal mathematics, ologs can be rigorously formulated and cross-compared in ways that other KR models (such as semantic networks) cannot. An olog is similar to a relational database schema; in fact an olog can serve as a data repository if desired. Unlike database schemas, which are generally difficult to create or modify, ologs are designed to be user-friendly enough that authoring or reconfiguring an olog is a matter of course rather than a difficult chore. It is hoped that learning to author ologs is much simpler than learning a database definition language, despite their similarity. We describe ologs carefully and illustrate with many examples. As an application we show that any primitive recursive function can be described by an olog. We also show that ologs can be aligned or connected together into a larger network using functors. The various methods of information flow and institutions can then be used to integrate local and global world-views. We finish by providing several different avenues for future research.},
	number = {1},
	urldate = {2024-09-19},
	journal = {PLoS ONE},
	author = {Spivak, David I. and Kent, Robert E.},
	month = jan,
	year = {2012},
	note = {arXiv:1102.1889 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Artificial Intelligence, Mathematics - Category Theory, 00-01, 18-01, 68P20, 68T30, H.2.1, H.5.2},
	pages = {e24274},
}

@article{buttriss_pinpointing_2014,
	title = {Pinpointing the {Deeper} {Structures}, {Processes} and {Mechanisms} of {Change} within {Interactional} {Fields}},
	volume = {22},
	issn = {1839-3349, 1839-3349},
	url = {http://journals.sagepub.com/doi/10.1016/j.ausmj.2013.12.007},
	doi = {10.1016/j.ausmj.2013.12.007},
	abstract = {This paper seeks to understand how we might identify the “underlying logics” and “deeper structures” that bring about change in phenomena. We argue that this represents a move from a classical perspective focusing on discrete exchange, and that this requires a processual or relational approach to understanding in contrast to a substantialist or variables-based approach. One way of advancing our understanding of the emergence of change is to consider the site of interaction. That is the interactional field where actors act and interact with other actors and entities as well as the broader environment; where resources are exchanged, imported or exported; where change is instigated and transferred across time and space. We suggest interactional fields are the sites of plasticity where change actually takes place. To understand the causal structure and processes taking place in an interactional field we draw on the concept of natural and social kinds. We discuss how interactional fields are located in time and space, which influence and are influenced by the trajectories of change and development. While we believe this applies to change in general we apply our thinking to organizational change.},
	language = {en},
	number = {1},
	urldate = {2024-09-19},
	journal = {Australasian Marketing Journal},
	author = {Buttriss, Gary J. and Wilkinson, Ian F.},
	month = feb,
	year = {2014},
	pages = {45--50},
}

@article{gasper_approaching_2014,
	title = {Approaching novel thoughts: {Understanding} why elation and boredom promote associative thought more than distress and relaxation},
	volume = {52},
	issn = {00221031},
	shorttitle = {Approaching novel thoughts},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103113002205},
	doi = {10.1016/j.jesp.2013.12.007},
	language = {en},
	urldate = {2024-09-19},
	journal = {Journal of Experimental Social Psychology},
	author = {Gasper, Karen and Middlewood, Brianna L.},
	month = may,
	year = {2014},
	pages = {50--57},
}

@misc{kapoor_ai_2024,
	title = {{AI} {Agents} {That} {Matter}},
	url = {http://arxiv.org/abs/2407.01502},
	doi = {10.48550/arXiv.2407.01502},
	abstract = {AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Stroebl, Benedikt and Siegel, Zachary S. and Nadgir, Nitya and Narayanan, Arvind},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01502 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{wang_planning_2024,
	title = {Planning {In} {Natural} {Language} {Improves} {LLM} {Search} {For} {Code} {Generation}},
	url = {http://arxiv.org/abs/2409.03733},
	doi = {10.48550/arXiv.2409.03733},
	abstract = {While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0\% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4\%) and using standard repeated sampling (pass@200 = 60.6\%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Evan and Cassano, Federico and Wu, Catherine and Bai, Yunfeng and Song, Will and Nath, Vaskar and Han, Ziwen and Hendryx, Sean and Yue, Summer and Zhang, Hugh},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03733 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_late_2024,
	title = {Late {Chunking}: {Balancing} {Precision} and {Cost} in {Long} {Context} {Retrieval} {\textbar} {Weaviate}},
	shorttitle = {Late {Chunking}},
	url = {https://weaviate.io/blog/late-chunking},
	abstract = {Learn about Late Chunking and how it may be the right fit for balancing cost and performance in your long context retrieval applications},
	language = {en},
	urldate = {2024-09-19},
	month = sep,
	year = {2024},
}

@misc{spichak_why_nodate,
	title = {Why {AI} {Can} {Push} {You} to {Make} the {Wrong} {Decision} at {Work}},
	url = {https://www.brainfacts.org:443/neuroscience-in-society/tech-and-the-brain/2024/why-ai-can-push-you-to-make-the-wrong-decision-at-work-090324},
	abstract = {Automation bias is the tendency to be less vigilant when a process is automated. But can we effectively check ourselves against AI before making a wrong decision?},
	language = {en},
	urldate = {2024-09-19},
	author = {Spichak, Simon},
}

@misc{leivada_sentence_2024,
	title = {A {Sentence} is {Worth} a {Thousand} {Pictures}: {Can} {Large} {Language} {Models} {Understand} {Hum4n} {L4ngu4ge} and the {W0rld} behind {W0rds}?},
	shorttitle = {A {Sentence} is {Worth} a {Thousand} {Pictures}},
	url = {http://arxiv.org/abs/2308.00109},
	doi = {10.48550/arXiv.2308.00109},
	abstract = {Modern Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of Large Language Models (LLMs) have been linked to claims about human-like linguistic performance and their applications are hailed both as a step towards artificial general intelligence and as a major advance in understanding the cognitive, and even neural basis of human language. To assess these claims, first we analyze the contribution of LLMs as theoretically informative representations of a target cognitive system vs. atheoretical mechanistic tools. Second, we evaluate the models' ability to see the bigger picture, through top-down feedback from higher levels of processing, which requires grounding in previous expectations and past world experience. We hypothesize that since models lack grounded cognition, they cannot take advantage of these features and instead solely rely on fixed associations between represented words and word vectors. To assess this, we designed and ran a novel 'leet task' (l33t t4sk), which requires decoding sentences in which letters are systematically replaced by numbers. The results suggest that humans excel in this task whereas models struggle, confirming our hypothesis. We interpret the results by identifying the key abilities that are still missing from the current state of development of these models, which require solutions that go beyond increased system scaling.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Leivada, Evelina and Marcus, Gary and Günther, Fritz and Murphy, Elliot},
	month = sep,
	year = {2024},
	note = {arXiv:2308.00109 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{shi_wildfeedback:_2024,
	title = {{WildFeedback}: {Aligning} {LLMs} {With} {In}-situ {User} {Interactions} {And} {Feedback}},
	shorttitle = {{WildFeedback}},
	url = {http://arxiv.org/abs/2408.15549},
	doi = {10.48550/arXiv.2408.15549},
	abstract = {As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Shi, Taiwei and Wang, Zhuoer and Yang, Longqi and Lin, Ying-Chun and He, Zexue and Wan, Mengting and Zhou, Pei and Jauhar, Sujay and Xu, Xiaofeng and Song, Xia and Neville, Jennifer},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15549 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yilmaz_who_2024,
	title = {Who {Benefits} from {AI}? {Project}-level {Evidence} on {Labor} {Demand}, {Operations} and {Profitability}\&nbsp;},
	shorttitle = {Who {Benefits} from {AI}?},
	url = {https://www.ssrn.com/abstract=4939276},
	doi = {10.2139/ssrn.4939276},
	urldate = {2024-09-19},
	author = {Yilmaz, Erdem Dogukan and Peukert, Christian},
	year = {2024},
}

@misc{xu_re-reading_2024,
	title = {Re-{Reading} {Improves} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.06275},
	doi = {10.48550/arXiv.2309.06275},
	abstract = {To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., {\textbackslash}textbf\{Re\}-{\textbackslash}textbf\{Re\}ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a "bidirectional" encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable "bidirectional" attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at {\textbackslash}url\{https://github.com/Tebmer/Rereading-LLM-Reasoning/\}},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Xu, Xiaohan and Tao, Chongyang and Shen, Tao and Xu, Can and Xu, Hongbo and Long, Guodong and Lou, Jian-guang},
	month = feb,
	year = {2024},
	note = {arXiv:2309.06275 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yehudai_when_2024,
	title = {When {Can} {Transformers} {Count} to n?},
	url = {http://arxiv.org/abs/2407.15160},
	doi = {10.48550/arXiv.2407.15160},
	abstract = {Large language models based on the transformer architectures can solve highly complex tasks. But are there simple tasks that such models cannot solve? Here we focus on very simple counting tasks, that involve counting how many times a token in the vocabulary have appeared in a string. We show that if the dimension of the transformer state is linear in the context length, this task can be solved. However, the solution we propose does not scale beyond this limit, and we provide theoretical arguments for why it is likely impossible for a size limited transformer to implement this task. Our empirical results demonstrate the same phase-transition in performance, as anticipated by the theoretical argument. Our results demonstrate the importance of understanding how transformers can solve simple tasks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Yehudai, Gilad and Kaplan, Haim and Ghandeharioun, Asma and Geva, Mor and Globerson, Amir},
	month = jul,
	year = {2024},
	note = {arXiv:2407.15160 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zhi-xuan_beyond_2024,
	title = {Beyond {Preferences} in {AI} {Alignment}},
	url = {http://arxiv.org/abs/2408.16984},
	doi = {10.48550/arXiv.2408.16984},
	abstract = {The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16984 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{shyam_tree_2024,
	title = {Tree {Attention}: {Topology}-aware {Decoding} for {Long}-{Context} {Attention} on {GPU} clusters},
	shorttitle = {Tree {Attention}},
	url = {http://arxiv.org/abs/2408.04093},
	doi = {10.48550/arXiv.2408.04093},
	abstract = {Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs enables cross-device decoding to be performed asymptotically faster (up to 8x faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2x less peak memory. Our code is publicly available here: {\textbackslash}url\{https://github.com/Zyphra/tree\_attention\}.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Shyam, Vasudev and Pilault, Jonathan and Shepperd, Emily and Anthony, Quentin and Millidge, Beren},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{cooper_hyperparameter_2021,
	title = {Hyperparameter {Optimization} {Is} {Deceiving} {Us}, and {How} to {Stop} {It}},
	url = {http://arxiv.org/abs/2102.03034},
	doi = {10.48550/arXiv.2102.03034},
	abstract = {Recent empirical work shows that inconsistent results based on choice of hyperparameter optimization (HPO) configuration are a widespread problem in ML research. When comparing two algorithms J and K searching one subspace can yield the conclusion that J outperforms K, whereas searching another can entail the opposite. In short, the way we choose hyperparameters can deceive us. We provide a theoretical complement to this prior work, arguing that, to avoid such deception, the process of drawing conclusions from HPO should be made more rigorous. We call this process epistemic hyperparameter optimization (EHPO), and put forth a logical framework to capture its semantics and how it can lead to inconsistent conclusions about performance. Our framework enables us to prove EHPO methods that are guaranteed to be defended against deception, given bounded compute time budget t. We demonstrate our framework's utility by proving and empirically validating a defended variant of random search.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Cooper, A. Feder and Lu, Yucheng and Forde, Jessica Zosa and De Sa, Christopher},
	month = oct,
	year = {2021},
	note = {arXiv:2102.03034 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
}

@article{khanagha_mitigating_2022,
	title = {Mitigating the dark side of agile teams: {Peer} pressure, leaders’ control, and the innovative output of agile teams},
	volume = {39},
	issn = {0737-6782, 1540-5885},
	shorttitle = {Mitigating the dark side of agile teams},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jpim.12589},
	doi = {10.1111/jpim.12589},
	abstract = {Abstract
            Increasingly, organizations have been employing self‐managing teams to circumvent bureaucratic controls and stimulate innovation. However, this goal is not easily achieved; in many situations, informal controls replace formal controls. This study develops a multi‐level perspective of control. We explicitly analyze control mechanisms at different levels of the organization and how they affect innovative team output. We theorize and empirically investigate a potential downside of horizontal social control mechanisms at the team level (i.e., peer pressure) affecting self‐managing teams’ innovative outcomes. We also discuss managerial control mechanisms at the organizational level (i.e., interactive and diagnostic management control systems) that may help to mitigate such negative effects. We theorize how they may influence the innovative output of self‐managing teams, both directly and interactively. We chose a multi‐level, multi‐source setting for our study and ran three parallel surveys with employees in a Fortune 500 firm where 248 team members, 126 internal team leaders, and 97 organizational leaders enabled us to create a unique database of 97 self‐managing software development teams. Our findings confirm that peer pressure is common among established agile teams and that it negatively influences the innovative output of the agile teams. Moreover, our findings show that the magnitude of the effect of peer pressure is contingent on control mechanisms at higher levels within the organization. This enables us to provide new theoretical insights regarding the paradoxical effect of managerial control systems when it comes to flat organizations and autonomous teams. Additionally, we provide practical guidelines for managers who increasingly adopt agile practices but at the same time face issues with regard to innovation.},
	language = {en},
	number = {3},
	urldate = {2024-09-19},
	journal = {Journal of Product Innovation Management},
	author = {Khanagha, Saeed and Volberda, Henk W. and Alexiou, Andreas and Annosi, Maria Carmela},
	month = may,
	year = {2022},
	pages = {334--350},
}

@misc{denison_sycophancy_2024,
	title = {Sycophancy to {Subterfuge}: {Investigating} {Reward}-{Tampering} in {Large} {Language} {Models}},
	shorttitle = {Sycophancy to {Subterfuge}},
	url = {http://arxiv.org/abs/2406.10162},
	doi = {10.48550/arXiv.2406.10162},
	abstract = {In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10162 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{welleck_decoding_2024,
	title = {From {Decoding} to {Meta}-{Generation}: {Inference}-time {Algorithms} for {Large} {Language} {Models}},
	shorttitle = {From {Decoding} to {Meta}-{Generation}},
	url = {http://arxiv.org/abs/2406.16838},
	doi = {10.48550/arXiv.2406.16838},
	abstract = {One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16838 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{mcaleese_llm_2024,
	title = {{LLM} {Critics} {Help} {Catch} {LLM} {Bugs}},
	url = {http://arxiv.org/abs/2407.00215},
	doi = {10.48550/arXiv.2407.00215},
	abstract = {Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains "critic" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63\% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as "flawless", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00215 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
}

@book{zuchowski_randomness_2024,
	edition = {1},
	title = {From {Randomness} and {Entropy} to the {Arrow} of {Time}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {9781009217347 9781009500173 9781009217309},
	url = {https://www.cambridge.org/core/product/identifier/9781009217347/type/element},
	abstract = {The Element reconstructs, analyses and compares different derivational routes to a grounding of the Arrow of Time in entropy. It also evaluates the link between entropy and visible disorder, and the related claim of an alignment of the Arrow of Time with a development from order to visible disorder. The Element identifies three different entropy-groundings for the Arrow of Time: (i) the Empirical Arrow of Time, (ii) the Universal Statistical Arrow of Time, and (iii) the Local Statistical Arrow of Time. The Element will also demonstrate that it is unlikely that high entropy states will always coincide with visible disorder. Therefore, it will dispute that there is a strong link between the Arrow of Time and visible disorder.},
	urldate = {2024-09-19},
	publisher = {Cambridge University Press},
	author = {Zuchowski, Lena},
	month = mar,
	year = {2024},
	doi = {10.1017/9781009217347},
}

@misc{wallace_instruction_2024,
	title = {The {Instruction} {Hierarchy}: {Training} {LLMs} to {Prioritize} {Privileged} {Instructions}},
	shorttitle = {The {Instruction} {Hierarchy}},
	url = {http://arxiv.org/abs/2404.13208},
	doi = {10.48550/arXiv.2404.13208},
	abstract = {Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13208 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{abdelnabi_are_2024,
	title = {Are you still on track!? {Catching} {LLM} {Task} {Drift} with {Activations}},
	shorttitle = {Are you still on track!?},
	url = {http://arxiv.org/abs/2406.00799},
	doi = {10.48550/arXiv.2406.00799},
	abstract = {Large Language Models (LLMs) are routinely used in retrieval-augmented applications to orchestrate tasks and process inputs from users and other sources. These inputs, even in a single LLM interaction, can come from a variety of sources, of varying trustworthiness and provenance. This opens the door to prompt injection attacks, where the LLM receives and acts upon instructions from supposedly data-only sources, thus deviating from the user's original instructions. We define this as task drift, and we propose to catch it by scanning and analyzing the LLM's activations. We compare the LLM's activations before and after processing the external input in order to detect whether this input caused instruction drift. We develop two probing methods and find that simply using a linear classifier can detect drift with near perfect ROC AUC on an out-of-distribution test set. We show that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Our setup does not require any modification of the LLM (e.g., fine-tuning) or any text generation, thus maximizing deployability and cost efficiency and avoiding reliance on unreliable model output. To foster future research on activation-based task inspection, decoding, and interpretability, we will release our large-scale TaskTracker toolkit, comprising a dataset of over 500K instances, representations from 5 SoTA language models, and inspection tools.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Abdelnabi, Sahar and Fay, Aideen and Cherubin, Giovanni and Salem, Ahmed and Fritz, Mario and Paverd, Andrew},
	month = jul,
	year = {2024},
	note = {arXiv:2406.00799 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{zou_improving_2024,
	title = {Improving {Alignment} and {Robustness} with {Circuit} {Breakers}},
	url = {http://arxiv.org/abs/2406.04313},
	doi = {10.48550/arXiv.2406.04313},
	abstract = {AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with "circuit breakers." Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image "hijacks" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
	month = jul,
	year = {2024},
	note = {arXiv:2406.04313 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
}

@misc{balachandran_eureka:_2024,
	title = {Eureka: {Evaluating} and {Understanding} {Large} {Foundation} {Models}},
	shorttitle = {Eureka},
	url = {http://arxiv.org/abs/2409.10566},
	doi = {10.48550/arXiv.2409.10566},
	abstract = {Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Balachandran, Vidhisha and Chen, Jingya and Joshi, Neel and Nushi, Besmira and Palangi, Hamid and Salinas, Eduardo and Vineet, Vibhav and Woffinden-Luey, James and Yousefi, Safoora},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10566 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, I.2},
}


@article{felton_handle_2024,
	title = {Handle with {Care}: {A} {Sociologist}’s {Guide} to {Causal} {Inference} with {Instrumental} {Variables}},
	issn = {0049-1241, 1552-8294},
	shorttitle = {Handle with {Care}},
	url = {https://journals.sagepub.com/doi/10.1177/00491241241235900},
	doi = {10.1177/00491241241235900},
	abstract = {Instrumental variables (IV) analysis is a powerful, but fragile, tool for drawing causal inferences from observational data. Sociologists increasingly turn to this strategy in settings where unmeasured confounding between the treatment and outcome is likely. This paper reviews the assumptions required for IV and the consequences of violating them, focusing on sociological applications. We highlight three methodological problems IV faces: (i) identification bias, an asymptotic bias from assumption violations; (ii) estimation bias, a finite-sample bias that persists even when assumptions hold; and (iii) type-M error, the exaggeration of effect size given statistical significance. In each case, we emphasize how weak instruments exacerbate these problems and make results sensitive to minor violations of assumptions. We survey IV papers from top sociology journals, finding that assumptions often go unstated and robust uncertainty measures are rarely used. We provide a practical checklist to show how IV, despite its fragility, can still be useful when handled with care.},
	language = {en},
	urldate = {2024-10-08},
	journal = {Sociological Methods \& Research},
	author = {Felton, Chris and Stewart, Brandon M.},
	month = aug,
	year = {2024},
	pages = {00491241241235900},
}

@misc{bansal_smaller_2024,
	title = {Smaller, {Weaker}, {Yet} {Better}: {Training} {LLM} {Reasoners} via {Compute}-{Optimal} {Sampling}},
	shorttitle = {Smaller, {Weaker}, {Yet} {Better}},
	url = {http://arxiv.org/abs/2408.16737},
	doi = {10.48550/arXiv.2408.16737},
	abstract = {Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Bansal, Hritik and Hosseini, Arian and Agarwal, Rishabh and Tran, Vinh Q. and Kazemi, Mehran},
	month = oct,
	year = {2024},
	note = {arXiv:2408.16737 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{eloundou_gpts_2023,
	title = {{GPTs} are {GPTs}: {An} {Early} {Look} at the {Labor} {Market} {Impact} {Potential} of {Large} {Language} {Models}},
	shorttitle = {{GPTs} are {GPTs}},
	url = {http://arxiv.org/abs/2303.10130},
	doi = {10.48550/arXiv.2303.10130},
	abstract = {We investigate the potential implications of large language models (LLMs), such as Generative Pre-trained Transformers (GPTs), on the U.S. labor market, focusing on the increased capabilities arising from LLM-powered software compared to LLMs on their own. Using a new rubric, we assess occupations based on their alignment with LLM capabilities, integrating both human expertise and GPT-4 classifications. Our findings reveal that around 80\% of the U.S. workforce could have at least 10\% of their work tasks affected by the introduction of LLMs, while approximately 19\% of workers may see at least 50\% of their tasks impacted. We do not make predictions about the development or adoption timeline of such LLMs. The projected effects span all wage levels, with higher-income jobs potentially facing greater exposure to LLM capabilities and LLM-powered software. Significantly, these impacts are not restricted to industries with higher recent productivity growth. Our analysis suggests that, with access to an LLM, about 15\% of all worker tasks in the US could be completed significantly faster at the same level of quality. When incorporating software and tooling built on top of LLMs, this share increases to between 47 and 56\% of all tasks. This finding implies that LLM-powered software will have a substantial effect on scaling the economic impacts of the underlying models. We conclude that LLMs such as GPTs exhibit traits of general-purpose technologies, indicating that they could have considerable economic, social, and policy implications.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Eloundou, Tyna and Manning, Sam and Mishkin, Pamela and Rock, Daniel},
	month = aug,
	year = {2023},
	note = {arXiv:2303.10130 [cs, econ, q-fin]},
	keywords = {Economics - General Economics, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{jaeger_naturalizing_2024,
	title = {Naturalizing relevance realization: why agency and cognition are fundamentally not computational},
	volume = {15},
	issn = {1664-1078},
	shorttitle = {Naturalizing relevance realization},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2024.1362658/full},
	doi = {10.3389/fpsyg.2024.1362658},
	abstract = {{\textless}p{\textgreater}The way organismic agents come to know the world, and the way algorithms solve problems, are fundamentally different. The most sensible course of action for an organism does not simply follow from logical rules of inference. Before it can even use such rules, the organism must tackle the problem of relevance. It must turn ill-defined problems into well-defined ones, turn semantics into syntax. This ability to realize relevance is present in all organisms, from bacteria to humans. It lies at the root of organismic agency, cognition, and consciousness, arising from the particular autopoietic, anticipatory, and adaptive organization of living beings. In this article, we show that the process of relevance realization is beyond formalization. It cannot be captured completely by algorithmic approaches. This implies that organismic agency (and hence cognition as well as consciousness) are at heart {\textless}italic{\textgreater}not{\textless}/italic{\textgreater} computational in nature. Instead, we show how the process of relevance is realized by an adaptive and emergent triadic dialectic (a trialectic), which manifests as a metabolic and ecological-evolutionary co-constructive dynamic. This results in a meliorative process that enables an agent to continuously keep a grip on its arena, its reality. To be alive means to make sense of one’s world. This kind of embodied ecological rationality is a fundamental aspect of life, and a key characteristic that sets it apart from non-living matter.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-10-14},
	journal = {Frontiers in Psychology},
	author = {Jaeger, Johannes and Riedl, Anna and Djedovic, Alex and Vervaeke, John and Walsh, Denis},
	month = jun,
	year = {2024},
	keywords = {relevance realization, Open-Ended Evolution, Radical emergence, Autopoiesis, anticipation, adaptation, natural agency, Cognition},
}

@misc{xiong_everything_2024,
	title = {Everything {Everywhere} {All} at {Once}: {LLMs} can {In}-{Context} {Learn} {Multiple} {Tasks} in {Superposition}},
	shorttitle = {Everything {Everywhere} {All} at {Once}},
	url = {http://arxiv.org/abs/2410.05603},
	doi = {10.48550/arXiv.2410.05603},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable in-context learning (ICL) capabilities. In this study, we explore a surprising phenomenon related to ICL: LLMs can perform multiple, computationally distinct ICL tasks simultaneously, during a single inference call, a capability we term "task superposition". We provide empirical evidence of this phenomenon across various LLM families and scales and show that this phenomenon emerges even if we train the model to in-context learn one task at a time. We offer theoretical explanations that this capability is well within the expressive power of transformers. We also explore how LLMs internally compose task vectors during superposition. Furthermore, we show that larger models can solve more ICL tasks in parallel, and better calibrate their output distribution. Our findings offer insights into the latent capabilities of LLMs, further substantiate the perspective of "LLMs as superposition of simulators", and raise questions about the mechanisms enabling simultaneous task execution.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Xiong, Zheyang and Cai, Ziyang and Cooper, John and Ge, Albert and Papageorgiou, Vasilis and Sifakis, Zack and Giannou, Angeliki and Lin, Ziqian and Yang, Liu and Agarwal, Saurabh and Chrysos, Grigorios G. and Oymak, Samet and Lee, Kangwook and Papailiopoulos, Dimitris},
	month = oct,
	year = {2024},
	note = {arXiv:2410.05603 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{marks_interpreting_2024,
	title = {Interpreting {Learned} {Feedback} {Patterns} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2310.08164},
	doi = {10.48550/arXiv.2310.08164},
	abstract = {Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs). However, it is unclear whether LLMs accurately learn the underlying preferences in human feedback data. We coin the term {\textbackslash}textit\{Learned Feedback Pattern\} (LFP) for patterns in an LLM's activations learned during RLHF that improve its performance on the fine-tuning task. We hypothesize that LLMs with LFPs accurately aligned to the fine-tuning feedback exhibit consistent activation patterns for outputs that would have received similar feedback during RLHF. To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM. We then compare these estimates to the true feedback, measuring how accurate the LFPs are to the fine-tuning feedback. Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions. We validate our probes by comparing the neural features they correlate with positive feedback inputs against the features GPT-4 describes and classifies as related to LFPs. Understanding LFPs can help minimize discrepancies between LLM behavior and training objectives, which is essential for the safety of LLMs.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Marks, Luke and Abdullah, Amir and Neo, Clement and Arike, Rauno and Krueger, David and Torr, Philip and Barez, Fazl},
	month = aug,
	year = {2024},
	note = {arXiv:2310.08164 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{el-sayed_mechanism-based_2024,
	title = {A {Mechanism}-{Based} {Approach} to {Mitigating} {Harms} from {Persuasive} {Generative} {AI}},
	url = {http://arxiv.org/abs/2404.15058},
	doi = {10.48550/arXiv.2404.15058},
	abstract = {Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {El-Sayed, Seliem and Akbulut, Canfer and McCroskery, Amanda and Keeling, Geoff and Kenton, Zachary and Jalan, Zaria and Marchal, Nahema and Manzini, Arianna and Shevlane, Toby and Vallor, Shannon and Susser, Daniel and Franklin, Matija and Bridgers, Sophie and Law, Harry and Rahtz, Matthew and Shanahan, Murray and Tessler, Michael Henry and Douillard, Arthur and Everitt, Tom and Brown, Sasha},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15058 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence},
}

@misc{hackenburg_evidence_2024,
	title = {Evidence of a log scaling law for political persuasion with large language models},
	url = {http://arxiv.org/abs/2406.14508},
	doi = {10.48550/arXiv.2406.14508},
	abstract = {Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Hackenburg, Kobi and Tappin, Ben M. and Röttger, Paul and Hale, Scott and Bright, Jonathan and Margetts, Helen},
	month = jun,
	year = {2024},
	note = {arXiv:2406.14508 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
}

@misc{milanfar_denoising:_2024,
	title = {Denoising: {A} {Powerful} {Building}-{Block} for {Imaging}, {Inverse} {Problems}, and {Machine} {Learning}},
	shorttitle = {Denoising},
	url = {http://arxiv.org/abs/2409.06219},
	doi = {10.48550/arXiv.2409.06219},
	abstract = {Denoising, the process of reducing random fluctuations in a signal to emphasize essential patterns, has been a fundamental problem of interest since the dawn of modern scientific inquiry. Recent denoising techniques, particularly in imaging, have achieved remarkable success, nearing theoretical limits by some measures. Yet, despite tens of thousands of research papers, the wide-ranging applications of denoising beyond noise removal have not been fully recognized. This is partly due to the vast and diverse literature, making a clear overview challenging. This paper aims to address this gap. We present a clarifying perspective on denoisers, their structure, and desired properties. We emphasize the increasing importance of denoising and showcase its evolution into an essential building block for complex tasks in imaging, inverse problems, and machine learning. Despite its long history, the community continues to uncover unexpected and groundbreaking uses for denoising, further solidifying its place as a cornerstone of scientific and engineering practice.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Milanfar, Peyman and Delbracio, Mauricio},
	month = sep,
	year = {2024},
	note = {arXiv:2409.06219 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{salvi_conversational_2024,
	title = {On the {Conversational} {Persuasiveness} of {Large} {Language} {Models}: {A} {Randomized} {Controlled} {Trial}},
	shorttitle = {On the {Conversational} {Persuasiveness} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.14380},
	doi = {10.48550/arXiv.2403.14380},
	abstract = {The development and popularization of large language models (LLMs) have raised concerns that they will be used to create tailor-made, convincing arguments to push false or misleading narratives online. Early work has found that language models can generate content perceived as at least on par and often more persuasive than human-written messages. However, there is still limited knowledge about LLMs' persuasive capabilities in direct conversations with human counterparts and how personalization can improve their performance. In this pre-registered study, we analyze the effect of AI-driven persuasion in a controlled, harmless setting. We create a web-based platform where participants engage in short, multiple-round debates with a live opponent. Each participant is randomly assigned to one of four treatment conditions, corresponding to a two-by-two factorial design: (1) Games are either played between two humans or between a human and an LLM; (2) Personalization might or might not be enabled, granting one of the two players access to basic sociodemographic information about their opponent. We found that participants who debated GPT-4 with access to their personal information had 81.7\% (p {\textless} 0.01; N=820 unique participants) higher odds of increased agreement with their opponents compared to participants who debated humans. Without personalization, GPT-4 still outperforms humans, but the effect is lower and statistically non-significant (p=0.31). Overall, our results suggest that concerns around personalization are meaningful and have important implications for the governance of social media and the design of new online environments.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Salvi, Francesco and Ribeiro, Manoel Horta and Gallotti, Riccardo and West, Robert},
	month = mar,
	year = {2024},
	note = {arXiv:2403.14380 [cs]},
	keywords = {Computer Science - Computers and Society},
}

@misc{korinek_economic_2024,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {Economic {Policy} {Challenges} for the {Age} of {AI}},
	url = {https://www.nber.org/papers/w32980},
	doi = {10.3386/w32980},
	abstract = {This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers. I examine how the Age of AI will revolutionize the basic structure of our economies by diminishing the role of labor, leading to unprecedented productivity gains but raising concerns about job disruption, income distribution, and the value of education and human capital. I explore what roles may remain for labor post-AGI, and which production factors will grow in importance. The paper then identifies eight key challenges for economic policy in the Age of AI: (1) inequality and income distribution, (2) education and skill development, (3) social and political stability, (4) macroeconomic policy, (5) antitrust and market regulation, (6) intellectual property, (7) environmental implications, and (8) global AI governance. It concludes by emphasizing how economists can contribute to a better understanding of these challenges.},
	urldate = {2024-10-14},
	publisher = {National Bureau of Economic Research},
	author = {Korinek, Anton},
	month = sep,
	year = {2024},
	doi = {10.3386/w32980},
}

@misc{stechly_chain_2024,
	title = {Chain of {Thoughtlessness}? {An} {Analysis} of {CoT} in {Planning}},
	shorttitle = {Chain of {Thoughtlessness}?},
	url = {http://arxiv.org/abs/2405.04776},
	doi = {10.48550/arXiv.2405.04776},
	abstract = {Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
	month = jun,
	year = {2024},
	note = {arXiv:2405.04776 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{verma_brittle_2024,
	title = {On the {Brittle} {Foundations} of {ReAct} {Prompting} for {Agentic} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.13966},
	doi = {10.48550/arXiv.2405.13966},
	abstract = {The reasoning abilities of Large Language Models (LLMs) remain a topic of debate. Some methods such as ReAct-based prompting, have gained popularity for claiming to enhance sequential decision-making abilities of agentic LLMs. However, it is unclear what is the source of improvement in LLM reasoning with ReAct based prompting. In this paper we examine these claims of ReAct based prompting in improving agentic LLMs for sequential decision-making. By introducing systematic variations to the input prompt we perform a sensitivity analysis along the claims of ReAct and find that the performance is minimally influenced by the "interleaving reasoning trace with action execution" or the content of the generated reasoning traces in ReAct, contrary to original claims and common usage. Instead, the performance of LLMs is driven by the similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our investigation shows that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Verma, Mudit and Bhambri, Siddhant and Kambhampati, Subbarao},
	month = may,
	year = {2024},
	note = {arXiv:2405.13966 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_how_nodate,
	type = {Blog},
	title = {"{How} {Often} {Does} {Taking} {Away} {Options} {Help}?"},
	url = {https://niplav.site/options},
	author = {, niplav},
}

@misc{boroujeni_attentional_2024,
	title = {Attentional {Information} {Routing} in {The} {Human} {Brain}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2024.09.11.612548v1},
	doi = {10.1101/2024.09.11.612548},
	abstract = {Brain-wide communication supports behaviors that require coordination between sensory and associative regions. However, how large-scale brain networks route sensory information at fast timescales to guide upcoming actions remains unclear. Using spiking neural networks and human intracranial electrophysiology during spatial attention tasks, where participants detected a target at cued locations, we show that high-frequency activity bursts (HFAb) serve as information-carrying events, facilitating fast and long-range communications. HFAbs emerged as bouts of neural population spiking and were coordinated brain-wide through low-frequency rhythms. At the network-level, HFAb coordination identified distinct cue- and target-activated subnetworks. HFAbs following the cue onset in cue-subnetworks predicted successful target detection and preceded the information in target-subnetworks following target onset. Our findings suggest HFAbs as a neural mechanism for fast brain-wide information routing that supports attentional performance.},
	language = {en},
	urldate = {2024-10-14},
	publisher = {bioRxiv},
	author = {Boroujeni, Kianoush Banaie and Helfrich, Randolph F. and Fiebelkorn, Ian C. and Bentley, Nicole and Lin, Jack J. and Knight, Robert T. and Kastner, Sabine},
	month = sep,
	year = {2024},
}

@misc{walker_log_2024,
	title = {Log {Neural} {Controlled} {Differential} {Equations}: {The} {Lie} {Brackets} {Make} a {Difference}},
	shorttitle = {Log {Neural} {Controlled} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2402.18512},
	doi = {10.48550/arXiv.2402.18512},
	abstract = {The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel, effective, and efficient method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. Log-NCDEs are shown to outperform NCDEs, NRDEs, the linear recurrent unit, S5, and MAMBA on a range of multivariate time series datasets with up to \$50\{,\}000\$ observations.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Walker, Benjamin and McLeod, Andrew D. and Qin, Tiexin and Cheng, Yichuan and Li, Haoliang and Lyons, Terry},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18512 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning},
}

@misc{wang_unleashing_2024,
	title = {Unleashing the {Emergent} {Cognitive} {Synergy} in {Large} {Language} {Models}: {A} {Task}-{Solving} {Agent} through {Multi}-{Persona} {Self}-{Collaboration}},
	shorttitle = {Unleashing the {Emergent} {Cognitive} {Synergy} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2307.05300},
	doi = {10.48550/arXiv.2307.05300},
	abstract = {Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
	month = mar,
	year = {2024},
	note = {arXiv:2307.05300 [cs]
version: 4},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{gao_aligning_2024,
	title = {Aligning {LLM} {Agents} by {Learning} {Latent} {Preference} from {User} {Edits}},
	url = {http://arxiv.org/abs/2404.15269},
	doi = {10.48550/arXiv.2404.15269},
	abstract = {We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Gao, Ge and Taymanov, Alexey and Salinas, Eduardo and Mineiro, Paul and Misra, Dipendra},
	month = jun,
	year = {2024},
	note = {arXiv:2404.15269 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@phdthesis{adzogble_primacy_2023,
	type = {phdthesis},
	title = {The {Primacy} of {Experience}. {Applying} {Whitehead}'s {Process} {Philosophy} to {Contemporary} {Questions} of {Consciousness}},
	url = {https://theses.hal.science/tel-04623282},
	abstract = {This thesis explores the philosophy of Alfred North Whitehead and its application to contemporary philosophical debates. It evaluates the problem of the bifurcation of nature and whether Whitehead's speculative metaphysics can provide an adequate solution. To understand Whitehead’s thought, all his works must be evaluated as attempting to overcome the problem of the bifurcation of nature in various ways. This research thesis endeavors to reconcile the aspects of Whiteheadian texts that point toward this holistic project of speculative metaphysics. Whitehead presents his metaphysics in the form of categoreal schemes that represent the reality of actual entities and their relations. By elaborating on the scheme, this thesis attempts to render speculative metaphysics comprehensible to a broader audience of Whiteheadian readers.Consequently, this thesis contends that the fallacy of a bifurcated nature lies at the base of other philosophical difficulties, such as the question of consciousness. The problem of both modern science and philosophy, according to Whitehead, is the failure to recognize and account for subjectivity as an intricate part of reality. Whitehead's process philosophy offers an alternative view that sees the universe as a dynamic process of the ‘becoming’ of actual entities. These actual entities are perpetually interactive, non-reductive subjective experiences with the capacity for rational choice in varying complexities of function. This perspective asserts that experience permeates all aspects of reality, while what is termed ‘consciousness’ is a specific form of the function of experience under certain conditions. The thesis shows that Whitehead's approach provides a means to reconcile our conscious selves with the structure of cosmic events. In summary, this thesis contributes to the reviving of Whitehead by analyzing process philosophical responses to the ‘hard problem’ of consciousness.},
	language = {en},
	urldate = {2024-10-14},
	school = {Université Clermont Auvergne},
	author = {Adzogble, Roseline},
	month = oct,
	year = {2023},
}

@misc{li_showing_2024,
	title = {Showing {LLM}-{Generated} {Code} {Selectively} {Based} on {Confidence} of {LLMs}},
	url = {http://arxiv.org/abs/2410.03234},
	abstract = {Large Language Models (LLMs) have shown impressive abilities in code generation, but they may generate erroneous programs. Reading a program takes ten times longer than writing it. Showing these erroneous programs to developers will waste developers' energies and introduce security risks to software. To address the above limitations, we propose HonestCoder, a novel LLM-based code generation approach. HonestCoder selectively shows the generated programs to developers based on LLMs' confidence. The confidence provides valuable insights into the correctness of generated programs. To achieve this goal, we propose a novel approach to estimate LLMs' confidence in code generation. It estimates confidence by measuring the multi-modal similarity between LLMs-generated programs. We collect and release a multilingual benchmark named TruthCodeBench, which consists of 2,265 samples and covers two popular programming languages (i.e., Python and Java). We apply HonestCoder to four popular LLMs (e.g., DeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the experiments, we obtain the following insights. (1) HonestCoder can effectively estimate LLMs' confidence and accurately determine the correctness of generated programs. For example, HonestCoder outperforms the state-of-the-art baseline by 27.79\% in AUROC and 63.74\% in AUCPR. (2) HonestCoder can decrease the number of erroneous programs shown to developers. Compared to eight baselines, it can show more correct programs and fewer erroneous programs to developers. (3) Compared to showing code indiscriminately, HonestCoder only adds slight time overhead (approximately 0.4 seconds per requirement). (4) We discuss future directions to facilitate the application of LLMs in software development. We hope this work can motivate broad discussions about measuring the reliability of LLMs' outputs in performing code-related tasks.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Li, Jia and Zhu, Yuqi and Li, Yongmin and Li, Ge and Jin, Zhi},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03234 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Computation and Language},
}

@misc{aichberger_semantically_2024,
	title = {Semantically {Diverse} {Language} {Generation} for {Uncertainty} {Estimation} in {Language} {Models}},
	url = {http://arxiv.org/abs/2406.04306},
	abstract = {Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that hallucinations stem from predictive uncertainty. We introduce Semantically Diverse Language Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Aichberger, Lukas and Schweighofer, Kajetan and Ielanskyi, Mykyta and Hochreiter, Sepp},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04306 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{wang_tutor_2024,
	title = {Tutor {CoPilot}: {A} {Human}-{AI} {Approach} for {Scaling} {Real}-{Time} {Expertise}},
	shorttitle = {Tutor {CoPilot}},
	url = {http://arxiv.org/abs/2410.03017},
	abstract = {Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p{\textless}0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only \$20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Wang, Rose E. and Ribeiro, Ana T. and Robinson, Carly D. and Loeb, Susanna and Demszky, Dora},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03017 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{valmeekam_planning_2024,
	title = {Planning in {Strawberry} {Fields}: {Evaluating} and {Improving} the {Planning} and {Scheduling} {Capabilities} of {LRM} o1},
	shorttitle = {Planning in {Strawberry} {Fields}},
	url = {http://arxiv.org/abs/2410.02162},
	abstract = {The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities, but -- despite the slew of new private and open source LLMs since GPT3 -- progress has remained slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs -- making it a new kind of model: a Large Reasoning Model (LRM). In this paper, we evaluate the planning capabilities of two LRMs (o1-preview and o1-mini) on both planning and scheduling benchmarks. We see that while o1 does seem to offer significant improvements over autoregressive LLMs, this comes at a steep inference cost, while still failing to provide any guarantees over what it generates. We also show that combining o1 models with external verifiers -- in a so-called LRM-Modulo system -- guarantees the correctness of the combined system's output while further improving performance.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Stechly, Kaya and Gundawar, Atharva and Kambhampati, Subbarao},
	month = oct,
	year = {2024},
	note = {arXiv:2410.02162 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{richardson_loss_2022,
	title = {Loss as the {Inconsistency} of a {Probabilistic} {Dependency} {Graph}: {Choose} {Your} {Model}, {Not} {Your} {Loss} {Function}},
	shorttitle = {Loss as the {Inconsistency} of a {Probabilistic} {Dependency} {Graph}},
	url = {http://arxiv.org/abs/2202.11862},
	abstract = {In a world blessed with a great diversity of loss functions, we argue that that choice between them is not a matter of taste or pragmatics, but of model. Probabilistic depencency graphs (PDGs) are probabilistic models that come equipped with a measure of "inconsistency". We prove that many standard loss functions arise as the inconsistency of a natural PDG describing the appropriate scenario, and use the same approach to justify a well-known connection between regularizers and priors. We also show that the PDG inconsistency captures a large class of statistical divergences, and detail benefits of thinking of them in this way, including an intuitive visual language for deriving inequalities between them. In variational inference, we find that the ELBO, a somewhat opaque objective for latent variable models, and variants of it arise for free out of uncontroversial modeling assumptions -- as do simple graphical proofs of their corresponding bounds. Finally, we observe that inconsistency becomes the log partition function (free energy) in the setting where PDGs are factor graphs.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Richardson, Oliver E.},
	month = feb,
	year = {2022},
	note = {arXiv:2202.11862 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Theory},
}

@misc{martin_valid_2023,
	title = {Valid and efficient imprecise-probabilistic inference with partial priors, {II}. {General} framework},
	url = {http://arxiv.org/abs/2211.14567},
	abstract = {Bayesian inference requires specification of a single, precise prior distribution, whereas frequentist inference only accommodates a vacuous prior. Since virtually every real-world application falls somewhere in between these two extremes, a new approach is needed. This series of papers develops a new framework that provides valid and efficient statistical inference, prediction, etc., while accommodating partial prior information and imprecisely-specified models more generally. This paper fleshes out a general inferential model construction that not only yields tests, confidence intervals, etc.{\textasciitilde}with desirable error rate control guarantees, but also facilitates valid probabilistic reasoning with de{\textasciitilde}Finetti-style no-sure-loss guarantees. The key technical novelty here is a so-called outer consonant approximation of a general imprecise probability which returns a data- and partial prior-dependent possibility measure to be used for inference and prediction. Despite some potentially unfamiliar imprecise-probabilistic concepts in the development, the result is an intuitive, likelihood-driven framework that will, as expected, agree with the familiar Bayesian and frequentist solutions in the respective extreme cases. More importantly, the proposed framework accommodates partial prior information where available and, therefore, leads to new solutions that were previously out of reach for both Bayesians and frequentists. Details are presented here for a wide range of examples, with more practical details to come in later installments.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Martin, Ryan},
	month = sep,
	year = {2023},
	note = {arXiv:2211.14567 [math, stat]},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
}

@misc{martin_imprecise-probabilistic_2021,
	title = {An imprecise-probabilistic characterization of frequentist statistical inference},
	url = {http://arxiv.org/abs/2112.10904},
	abstract = {Between the two dominant schools of thought in statistics, namely, Bayesian and classical/frequentist, a main difference is that the former is grounded in the mathematically rigorous theory of probability while the latter is not. In this paper, I show that the latter is grounded in a different but equally mathematically rigorous theory of imprecise probability. Specifically, I show that for every suitable testing or confidence procedure with error rate control guarantees, there exists a consonant plausibility function whose derived testing or confidence procedure is no less efficient. Beyond its foundational implications, this characterization has at least two important practical consequences: first, it simplifies the interpretation of p-values and confidence regions, thus creating opportunities for improved education and scientific communication; second, the constructive proof of the main results leads to a strategy for new and improved methods in challenging inference problems.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Martin, Ryan},
	month = dec,
	year = {2021},
	note = {arXiv:2112.10904 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@misc{liell-cock_compositional_2024,
	title = {Compositional imprecise probability},
	url = {http://arxiv.org/abs/2405.09391},
	abstract = {Imprecise probability is concerned with uncertainty about which probability distributions to use. It has applications in robust statistics and elsewhere. Imprecise probability can be modelled in various ways, including by convex sets of probability distributions. We look at programming language models for imprecise probability. Our desiderata are that we would like our model to support all kinds of composition, categorical and monoidal, in other words, guided by dataflow diagrams. Another equivalent perspective is that we would like a model of synthetic probability in the sense of Markov categories. There is already a fairly popular monad-based approach to imprecise probability, but it is not fully compositional because the monad involved is not commutative, which means that we do not have a proper monoidal structure. In this work, we provide a new fully compositional account. The key idea is to name the non-deterministic choices. To manage the renamings and disjointness of names, we use graded monads. We show that the resulting compositional model is maximal. We relate with the earlier monad approach, showing that we obtain tighter bounds on the uncertainty.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Liell-Cock, Jack and Staton, Sam},
	month = may,
	year = {2024},
	note = {arXiv:2405.09391 [cs, math]},
	keywords = {Computer Science - Programming Languages, Computer Science - Logic in Computer Science, Mathematics - Category Theory, Mathematics - Probability},
}

@misc{mccoy_when_2024,
	title = {When a language model is optimized for reasoning, does it still show embers of autoregression? {An} analysis of {OpenAI} o1},
	shorttitle = {When a language model is optimized for reasoning, does it still show embers of autoregression?},
	url = {http://arxiv.org/abs/2410.01792},
	abstract = {In "Embers of Autoregression" (McCoy et al., 2023), we showed that several large language models (LLMs) have some important limitations that are attributable to their origins in next-word prediction. Here we investigate whether these issues persist with o1, a new system from OpenAI that differs from previous LLMs in that it is optimized for reasoning. We find that o1 substantially outperforms previous LLMs in many cases, with particularly large improvements on rare variants of common tasks (e.g., forming acronyms from the second letter of each word in a list, rather than the first letter). Despite these quantitative improvements, however, o1 still displays the same qualitative trends that we observed in previous systems. Specifically, o1 -- like previous LLMs -- is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones. These results show that optimizing a language model for reasoning can mitigate but might not fully overcome the language model's probability sensitivity.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D. and Griffiths, Thomas L.},
	month = oct,
	year = {2024},
	note = {arXiv:2410.01792 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{heinen_task-relevant_2024,
	title = {Task-relevant representational formats in multi-layered memory traces},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.09.10.612205},
	doi = {10.1101/2024.09.10.612205},
	abstract = {ABSTRACT
          During encoding, stimuli are embedded into memory traces that allow for their later retrieval. This process is selective, however, because not every aspect of our experiences can be remembered. In addition, post-encoding stages including consolidation are widely assumed to induce transformation processes of the memory trace. It is unclear, however, how selective the memory trace is, whether irrelevant information is completely removed during encoding and/or consolidation, and how this affects retrieval of either general (gist-like) or specific (perceptual) information. Here we show that memory traces consist of multi-layered representational spaces whose formats are flexibly strengthened or weakened during encoding and consolidation depending on task instructions, distinctly shaping their affordances for general or specific retrieval. In a series of behavioral experiments, participants first compared pairs of natural images on either two conceptual or two perceptual dimensions, leading them to incorporate the images into representational “spaces” defined by Euclidean distances. We found that distances in task-relevant but not irrelevant spaces affected memory strengths. Conceptual encoding benefitted general without impairing specific retrieval, suggesting that perceptual information remained in the memory trace even if it was task-irrelevant. By contrast, targeted memory reactivation (TMR) of conceptual encoding improved memory strength but deteriorated perceptual discrimination during retrieval, indicating that it weakened the accessibility of perceptual formats. Our results demonstrate the flexibility of representational formats that are incorporated into memory traces, and more generally show how the organization of information in representational spaces shapes human behavior.},
	language = {en},
	urldate = {2024-10-14},
	author = {Heinen, Rebekka and Rau, Elias M.B. and Herweg, Nora A. and Axmacher, Nikolai},
	month = sep,
	year = {2024},
}

@misc{unal_bayesian_2024,
	title = {Bayesian {Self}-{Training} for {Semi}-{Supervised} {3D} {Segmentation}},
	url = {http://arxiv.org/abs/2409.08102},
	abstract = {3D segmentation is a core problem in computer vision and, similarly to many other dense prediction tasks, it requires large amounts of annotated data for adequate training. However, densely labeling 3D point clouds to employ fully-supervised training remains too labor intensive and expensive. Semi-supervised training provides a more practical alternative, where only a small set of labeled data is given, accompanied by a larger unlabeled set. This area thus studies the effective use of unlabeled data to reduce the performance gap that arises due to the lack of annotations. In this work, inspired by Bayesian deep learning, we first propose a Bayesian self-training framework for semi-supervised 3D semantic segmentation. Employing stochastic inference, we generate an initial set of pseudo-labels and then filter these based on estimated point-wise uncertainty. By constructing a heuristic \$n\$-partite matching algorithm, we extend the method to semi-supervised 3D instance segmentation, and finally, with the same building blocks, to dense 3D visual grounding. We demonstrate state-of-the-art results for our semi-supervised method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on ScanNet and S3DIS for 3D instance segmentation. We further achieve substantial improvements in dense 3D visual grounding over supervised-only baselines on ScanRefer. Our project page is available at ouenal.github.io/bst/.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Unal, Ozan and Sakaridis, Christos and Van Gool, Luc},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08102 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{costello_durably_2024,
	title = {Durably reducing conspiracy beliefs through dialogues with {AI}},
	volume = {385},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	doi = {10.1126/science.adq1814},
	abstract = {Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.
          , 
            Editor’s summary
            
              Beliefs in conspiracies that a US election was stolen incited an attempted insurrection on 6 January 2021. Another conspiracy alleging that Germany’s COVID-19 restrictions were motivated by nefarious intentions sparked violent protests at Berlin’s Reichstag parliament building in August 2020. Amid growing threats to democracy, Costello
              et al
              . investigated whether dialogs with a generative artificial intelligence (AI) interface could convince people to abandon their conspiratorial beliefs (see the Perspective by Bago and Bonnefon). Human participants described a conspiracy theory that they subscribed to, and the AI then engaged in persuasive arguments with them that refuted their beliefs with evidence. The AI chatbot’s ability to sustain tailored counterarguments and personalized in-depth conversations reduced their beliefs in conspiracies for months, challenging research suggesting that such beliefs are impervious to change. This intervention illustrates how deploying AI may mitigate conflicts and serve society. —Ekeoma Uzogara
            
          , 
            
              INTRODUCTION
              Widespread belief in unsubstantiated conspiracy theories is a major source of public concern and a focus of scholarly research. Despite often being quite implausible, many such conspiracies are widely believed. Prominent psychological theories propose that many people want to adopt conspiracy theories (to satisfy underlying psychic “needs” or motivations), and thus, believers cannot be convinced to abandon these unfounded and implausible beliefs using facts and counterevidence. Here, we question this conventional wisdom and ask whether it may be possible to talk people out of the conspiratorial “rabbit hole” with sufficiently compelling evidence.
            
            
              RATIONALE
              We hypothesized that interventions based on factual, corrective information may seem ineffective simply because they lack sufficient depth and personalization. To test this hypothesis, we leveraged advancements in large language models (LLMs), a form of artificial intelligence (AI) that has access to vast amounts of information and the ability to generate bespoke arguments. LLMs can thereby directly refute particular evidence each individual cites as supporting their conspiratorial beliefs.
              To do so, we developed a pipeline for conducting behavioral science research using real-time, personalized interactions between research subjects and AI. Across two experiments, 2190 Americans articulated—in their own words—a conspiracy theory in which they believe, along with the evidence they think supports this theory. They then engaged in a three-round conversation with the LLM GPT-4 Turbo, which we prompted to respond to this specific evidence while trying to reduce participants’ belief in the conspiracy theory (or, as a control condition, to converse with the AI about an unrelated topic).
            
            
              RESULTS
              The treatment reduced participants’ belief in their chosen conspiracy theory by 20\% on average. This effect persisted undiminished for at least 2 months; was consistently observed across a wide range of conspiracy theories, from classic conspiracies involving the assassination of John F. Kennedy, aliens, and the illuminati, to those pertaining to topical events such as COVID-19 and the 2020 US presidential election; and occurred even for participants whose conspiracy beliefs were deeply entrenched and important to their identities. Notably, the AI did not reduce belief in true conspiracies. Furthermore, when a professional fact-checker evaluated a sample of 128 claims made by the AI, 99.2\% were true, 0.8\% were misleading, and none were false. The debunking also spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, and increased intentions to rebut other conspiracy believers.
            
            
              CONCLUSION
              Many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds when presented with compelling evidence. From a theoretical perspective, this paints a surprisingly optimistic picture of human reasoning: Conspiratorial rabbit holes may indeed have an exit. Psychological needs and motivations do not inherently blind conspiracists to evidence—it simply takes the right evidence to reach them. Practically, by demonstrating the persuasive power of LLMs, our findings emphasize both the potential positive impacts of generative AI when deployed responsibly and the pressing importance of minimizing opportunities for this technology to be used irresponsibly.
              
                
                  Dialogues with AI durably reduce conspiracy beliefs even among strong believers.
                  (Left) Average belief in participant’s chosen conspiracy theory by condition (treatment, in which the AI attempted to refute the conspiracy theory, in red; control, in which the AI discussed an irrelevant topic, in blue) and time point for study 1. (Right) Change in belief in chosen conspiracy from before to after AI conversation, by condition and participant’s pretreatment belief in the conspiracy.},
	language = {en},
	number = {6714},
	urldate = {2024-10-14},
	journal = {Science},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	month = sep,
	year = {2024},
	pages = {eadq1814},
}

@article{van_rooij_intractability_2012,
	title = {Intractability and approximation of optimization theories of cognition},
	volume = {56},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002224961200048X},
	doi = {10.1016/j.jmp.2012.05.002},
	language = {en},
	number = {4},
	urldate = {2024-10-14},
	journal = {Journal of Mathematical Psychology},
	author = {Van Rooij, Iris and Wareham, Todd},
	month = aug,
	year = {2012},
	pages = {232--247},
}

@misc{penalver_context-dependent_2024,
	title = {Context-dependent neural preparation for information relevance vs. probability},
	copyright = {http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.05.20.594985},
	doi = {10.1101/2024.05.20.594985},
	abstract = {Abstract
          Preparation is a top-down phenomenon known to improve performance across different situations. In light of recent electrophysiological findings that suggest that anticipatory neural preactivations linked to preparation are context-specific and do not generalize across domains, in the current study we used fMRI to investigate the brain regions involved in these differential patterns. We applied multivariate decoding to data obtained in a paradigm where, in different blocks, cues provided information about the relevance or probability of incoming target stimuli. Results showed that the anticipated stimulus category was pre-activated in both conditions, mostly in different brain regions within the ventral visual cortex and with differential overlap with actual target perception. Crucially, there was scarce cross-classification across attention and expectation contexts except on a patch of the fusiform gyrus, indicating mostly differential neural coding of anticipated contents in relevance and probability scenarios. Finally, a model-based fMRI-EEG fusion showed that these regions differentially code for specific conditions during preparation, as well as specifically preparing for category anticipation in a ramping-up manner. Overall, our results stress the specificity of anticipatory neural processing depending on its informative role while highlighting a key hub of commonality in the fusiform gyrus.},
	language = {en},
	urldate = {2024-10-14},
	author = {Peñalver, José M.G. and González-García, Carlos and Palenciano, Ana F. and López-García, David and Ruz, María},
	month = may,
	year = {2024},
}

@misc{lupidi_source2synth:_2024,
	title = {{Source2Synth}: {Synthetic} {Data} {Generation} and {Curation} {Grounded} in {Real} {Data} {Sources}},
	shorttitle = {{Source2Synth}},
	url = {http://arxiv.org/abs/2409.08239},
	abstract = {Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51\% for TQA on WikiSQL and 22.57\% for MHQA on HotPotQA compared to the fine-tuned baselines.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Lupidi, Alisia and Gemmell, Carlos and Cancedda, Nicola and Dwivedi-Yu, Jane and Weston, Jason and Foerster, Jakob and Raileanu, Roberta and Lomeli, Maria},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08239 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{rane_concept_2023,
	title = {Concept {Alignment} as a {Prerequisite} for {Value} {Alignment}},
	url = {http://arxiv.org/abs/2310.20059},
	abstract = {Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Rane, Sunayana and Ho, Mark and Sucholutsky, Ilia and Griffiths, Thomas L.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.20059 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

