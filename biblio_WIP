# verbose-funicular


@book{morgan_counterfactuals_2015,
	address = {New York, NY},
	edition = {Second Edition},
	series = {Analytical methods for social research},
	title = {Counterfactuals and causal inference: methods and principles for social research},
	isbn = {9781107065079 9781107694163},
	shorttitle = {Counterfactuals and causal inference},
	abstract = {"In this second edition of Counterfactuals and Causal Inference, completely revised and expanded, the essential features of the counterfactual approach to observational data analysis are presented with examples from the social, demographic, and health sciences. Alternative estimation techniques are first introduced using both the potential outcome model and causal graphs; after which, conditioning techniques, such as matching and regression, are presented from a potential outcomes perspective. For research scenarios in which important determinants of causal exposure are unobserved, alternative techniques, such as instrumental variable estimators, longitudinal methods, and estimation via causal mechanisms, are then presented. The importance of causal effect heterogeneity is stressed throughout the book, and the need for deep causal explanation via mechanisms is discussed"--},
	publisher = {Cambridge University Press},
	author = {Morgan, Stephen L. and Winship, Christopher},
	year = {2015},
	keywords = {Social sciences, Research, Social sciences, Methodology, Causation, MATHEMATICS / Probability \& Statistics / General},
}

@misc{christiano_finding_2022,
	title = {Finding gliders in the game of life},
	url = {https://ai-alignment.com/finding-gliders-in-the-game-of-life-b7c93b51079d},
	abstract = {Walking through a simple concrete example of ARC’s approach to ELK based on mechanistic anomaly detection.},
	language = {en},
	author = {Christiano, Paul},
	month = dec,
	year = {2022},
}

@misc{christiano_paul_notitle_2022,
	url = {https://www.lesswrong.com/posts/JLyWP2Y9LAruR2gi9/can-we-efficiently-distinguish-different-mechanisms},
	abstract = {(This post is an elaboration on “tractability of discrimination” as introduced in section III of Can we efficiently explain model behaviors? For an o…},
	language = {en},
	journal = {"Can we efficiently distinguish different mechanisms?"},
	author = {{Christiano, Paul}},
	month = dec,
	year = {2022},
}

@book{pearl_book_2018,
	address = {New York},
	edition = {First edition},
	title = {The book of why: the new science of cause and effect},
	isbn = {9780465097609},
	shorttitle = {The book of why},
	abstract = {"Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world"--},
	publisher = {Basic Books},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2018},
	keywords = {Causation, Inference},
}

@misc{baez_what_2024,
	title = {What is {Entropy}?},
	url = {http://arxiv.org/abs/2409.09232},
	doi = {10.48550/arXiv.2409.09232},
	abstract = {This short book is an elementary course on entropy, leading up to a calculation of the entropy of hydrogen gas at standard temperature and pressure. Topics covered include information, Shannon entropy and Gibbs entropy, the principle of maximum entropy, the Boltzmann distribution, temperature and coolness, the relation between entropy, expected energy and temperature, the equipartition theorem, the partition function, the relation between expected energy, free energy and entropy, the entropy of a classical harmonic oscillator, the entropy of a classical particle in a box, and the entropy of a classical ideal gas.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Baez, John C.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09232 [cond-mat, physics:math-ph]},
	keywords = {Condensed Matter - Statistical Mechanics, Mathematical Physics},
}

@misc{siegel_core-bench:_2024,
	title = {{CORE}-{Bench}: {Fostering} the {Credibility} of {Published} {Research} {Through} a {Computational} {Reproducibility} {Agent} {Benchmark}},
	shorttitle = {{CORE}-{Bench}},
	url = {http://arxiv.org/abs/2409.11363},
	doi = {10.48550/arXiv.2409.11363},
	abstract = {AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21\% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Siegel, Zachary S. and Kapoor, Sayash and Nagdir, Nitya and Stroebl, Benedikt and Narayanan, Arvind},
	month = sep,
	year = {2024},
	note = {arXiv:2409.11363 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@misc{nemecek_coinductive_2023,
	title = {Coinductive guide to inductive transformer heads},
	url = {http://arxiv.org/abs/2302.01834},
	doi = {10.48550/arXiv.2302.01834},
	abstract = {We argue that all building blocks of transformer models can be expressed with a single concept: combinatorial Hopf algebra. Transformer learning emerges as a result of the subtle interplay between the algebraic and coalgebraic operations of the combinatorial Hopf algebra. Viewed through this lens, the transformer model becomes a linear time-invariant system where the attention mechanism computes a generalized convolution transform and the residual stream serves as a unit impulse. Attention-only transformers then learn by enforcing an invariant between these two paths. We call this invariant Hopf coherence. Due to this, with a degree of poetic license, one could call combinatorial Hopf algebras "tensors with a built-in loss function gradient". This loss function gradient occurs within the single layers and no backward pass is needed. This is in contrast to automatic differentiation which happens across the whole graph and needs a explicit backward pass. This property is the result of the fact that combinatorial Hopf algebras have the surprising property of calculating eigenvalues by repeated squaring.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Nemecek, Adam},
	month = feb,
	year = {2023},
	note = {arXiv:2302.01834 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{liu_retrievalattention:_2024,
	title = {{RetrievalAttention}: {Accelerating} {Long}-{Context} {LLM} {Inference} via {Vector} {Retrieval}},
	shorttitle = {{RetrievalAttention}},
	url = {http://arxiv.org/abs/2409.10516},
	doi = {10.48550/arXiv.2409.10516},
	abstract = {Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference latency and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to use approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieves the most relevant ones with vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation shows that RetrievalAttention only needs to access 1--3\% of data while maintaining high model accuracy. This leads to significant reduction in the inference cost of long-context LLMs with much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Liu, Di and Chen, Meng and Lu, Baotong and Jiang, Huiqiang and Han, Zhenhua and Zhang, Qianxi and Chen, Qi and Zhang, Chengruidong and Ding, Bailu and Zhang, Kai and Chen, Chen and Yang, Fan and Yang, Yuqing and Qiu, Lili},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10516 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{imgur_attention_nodate,
	title = {Attention mechanism of the {Transformer}},
	url = {https://imgur.com/gallery/vuw15aL},
	abstract = {Discover topics like transformer, attention, and the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users like JaviiiiAbellan.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Imgur},
	author = {{Imgur}},
}

@misc{imgur_multihead_nodate,
	title = {{MultiHead} {Attention}},
	url = {https://imgur.com/gallery/FBQqrxw},
	abstract = {Discover topics like multihead, transformer, attention, and the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and so much more from users like JaviiiiAbellan.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Imgur},
	author = {{Imgur}},
}

@misc{li_chain_2024,
	title = {Chain of {Thought} {Empowers} {Transformers} to {Solve} {Inherently} {Serial} {Problems}},
	url = {http://arxiv.org/abs/2402.12875},
	doi = {10.48550/arXiv.2402.12875},
	abstract = {Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length \$n\$, previous works have shown that constant-depth transformers with finite precision \${\textbackslash}mathsf\{poly\}(n)\$ embedding size can only solve problems in \${\textbackslash}mathsf\{TC\}{\textasciicircum}0\$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in \${\textbackslash}mathsf\{AC\}{\textasciicircum}0\$, a proper subset of \$ {\textbackslash}mathsf\{TC\}{\textasciicircum}0\$. However, with \$T\$ steps of CoT, constant-depth transformers using constant-bit precision and \$O({\textbackslash}log n)\$ embedding size can solve any problem solvable by boolean circuits of size \$T\$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
	month = may,
	year = {2024},
	note = {arXiv:2402.12875 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Complexity, Statistics - Machine Learning},
}

@misc{park_geometry_2024,
	title = {The {Geometry} of {Categorical} and {Hierarchical} {Concepts} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.01506},
	doi = {10.48550/arXiv.2406.01506},
	abstract = {Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as \{'mammal', 'bird', 'reptile', 'fish'\}, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate these theoretical results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from WordNet.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01506 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{scholze_lectures_2019,
	title = {Lectures on {Analytic} {Geometry}},
	url = {https://www.math.uni-bonn.de/people/scholze/Analytic.pdf},
	author = {Scholze, Peter},
	month = oct,
	year = {2019},
}

@misc{wu_sgformer:_2024,
	title = {{SGFormer}: {Single}-{Layer} {Graph} {Transformers} with {Approximation}-{Free} {Linear} {Complexity}},
	shorttitle = {{SGFormer}},
	url = {http://arxiv.org/abs/2409.09007},
	doi = {10.48550/arXiv.2409.09007},
	abstract = {Learning representations on large graphs is a long-standing challenge due to the inter-dependence nature. Transformers recently have shown promising performance on small graphs thanks to its global attention for capturing all-pair interactions beyond observed structures. Existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated architectures by stacking deep attention-based propagation layers. In this paper, we attempt to evaluate the necessity of adopting multi-layer attentions in Transformers on graphs, which considerably restricts the efficiency. Specifically, we analyze a generic hybrid propagation layer, comprised of all-pair attention and graph-based propagation, and show that multi-layer propagation can be reduced to one-layer propagation, with the same capability for representation learning. It suggests a new technical path for building powerful and efficient Transformers on graphs, particularly through simplifying model architectures without sacrificing expressiveness. As exemplified by this work, we propose a Simplified Single-layer Graph Transformers (SGFormer), whose main component is a single-layer global attention that scales linearly w.r.t. graph sizes and requires none of any approximation for accommodating all-pair interactions. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M, yielding orders-of-magnitude inference acceleration over peer Transformers on medium-sized graphs, and demonstrates competitiveness with limited labeled data.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wu, Qitian and Yang, Kai and Zhang, Hengrui and Wipf, David and Yan, Junchi},
	month = sep,
	year = {2024},
	note = {arXiv:2409.09007 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{ashktorab_emerging_2024,
	title = {Emerging {Reliance} {Behaviors} in {Human}-{AI} {Text} {Generation}: {Hallucinations}, {Data} {Quality} {Assessment}, and {Cognitive} {Forcing} {Functions}},
	shorttitle = {Emerging {Reliance} {Behaviors} in {Human}-{AI} {Text} {Generation}},
	url = {http://arxiv.org/abs/2409.08937},
	doi = {10.48550/arXiv.2409.08937},
	abstract = {In this paper, we investigate the impact of hallucinations and cognitive forcing functions in human-AI collaborative text generation tasks, focusing on the use of Large Language Models (LLMs) to assist in generating high-quality conversational data. LLMs require data for fine-tuning, a crucial step in enhancing their performance. In the context of conversational customer support, the data takes the form of a conversation between a human customer and an agent and can be generated with an AI assistant. In our inquiry, involving 11 users who each completed 8 tasks, resulting in a total of 88 tasks, we found that the presence of hallucinations negatively impacts the quality of data. We also find that, although the cognitive forcing function does not always mitigate the detrimental effects of hallucinations on data quality, the presence of cognitive forcing functions and hallucinations together impacts data quality and influences how users leverage the AI responses presented to them. Our analysis of user behavior reveals distinct patterns of reliance on AI-generated responses, highlighting the importance of managing hallucinations in AI-generated content within conversational AI contexts.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ashktorab, Zahra and Pan, Qian and Geyer, Werner and Desmond, Michael and Danilevsky, Marina and Johnson, James M. and Dugan, Casey and Bachman, Michelle},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08937 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@misc{van_rooij_reclaiming_2023,
	title = {Reclaiming {AI} as a theoretical tool for cognitive science},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	url = {https://osf.io/4cbuv},
	doi = {10.31234/osf.io/4cbuv},
	abstract = {The idea that human cognition is, or can be understood as, a form of computation is a useful conceptual tool for cognitive science. It was a foundational assumption during the birth of cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its contributing fields. One conception of AI in this context is as a provider of computational tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory building in cognitive science. The contemporary field of AI, however, has taken the theoretical possibility of explaining human cognition as a form of computation to imply the practical feasibility of realising human(-like or -level) cognition in factual computational systems; and, the field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, creating systems with human(-like or -level) cognition is intrinsically computationally intractable. This means that any factual AI systems created in the short-run are at best decoys. When we think these systems capture something deep about ourselves and our thinking, we induce distorted and impoverished images of ourselves and our cognition. In other words, AI in current practice is deteriorating our theoretical understanding of cognition rather than advancing and enhancing it. The situation could be remediated by releasing the grip of the currently dominant view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of the past (and present) that brought us to where we are today.},
	urldate = {2024-09-19},
	author = {Van Rooij, Iris and Guest, Olivia and Adolfi, Federico G and De Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
	month = aug,
	year = {2023},
}

@misc{bowman_checklist:_2024,
	title = {The {Checklist}: {What} {Succeeding} at {AI} {Safety} {Will} {Involve}},
	shorttitle = {The {Checklist}},
	url = {https://sleepinyourhat.github.io/checklist/},
	abstract = {Sep 3, 2024 Preface This piece reflects my current best guess at the major goals that Anthropic (or another similarly positioned AI developer) will need to …},
	language = {en},
	author = {Bowman, Sam},
	month = sep,
	year = {2024},
}

@misc{brown_large_2024,
	title = {Large {Language} {Monkeys}: {Scaling} {Inference} {Compute} with {Repeated} {Sampling}},
	shorttitle = {Large {Language} {Monkeys}},
	url = {http://arxiv.org/abs/2407.21787},
	doi = {10.48550/arXiv.2407.21787},
	abstract = {Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling by increasing the number of generated samples. Across multiple tasks and models, we observe that coverage - the fraction of problems solved by any attempt - scales with the number of samples over four orders of magnitude. In domains like coding and formal proofs, where all answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-V2-Coder-Instruct increases from 15.9\% with one sample to 56\% with 250 samples, outperforming the single-attempt state-of-the-art of 43\% which uses more capable frontier models. Moreover, using current API pricing, amplifying the cheaper DeepSeek model with five samples is more cost-effective and solves more issues than paying a premium for one sample from GPT-4o or Claude 3.5 Sonnet. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. Finally, we find that identifying correct samples out of many generations remains an important direction for future research in domains without automatic verifiers. When solving math word problems from GSM8K and MATH, coverage with Llama-3 models grows to over 95\% with 10,000 samples. However, common methods to pick correct solutions from a sample collection, such as majority voting or reward models, plateau beyond several hundred samples and fail to fully scale with the sample budget.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and Ré, Christopher and Mirhoseini, Azalia},
	month = sep,
	year = {2024},
	note = {arXiv:2407.21787 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{sevilla_training_2024,
	title = {Training {Compute} of {Frontier} {AI} {Models} {Grows} by 4-5x per {Year}},
	url = {https://epochai.org/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year},
	abstract = {Our expanded AI model database shows that the compute used to train recent models grew 4-5x yearly from 2010 to May 2024. We find similar growth in frontier models, recent large language models, and models from leading companies.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Sevilla, Jaime},
	month = may,
	year = {2024},
}

@misc{cottier_how_2024,
	title = {How {Much} {Does} {It} {Cost} to {Train} {Frontier} {AI} {Models}?},
	url = {https://epochai.org/blog/how-much-does-it-cost-to-train-frontier-ai-models},
	abstract = {The cost of training frontier AI models has grown by a factor of 2 to 3x per year for the past eight years, suggesting that the largest models will cost over a billion dollars by 2027.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Cottier, Ben},
	month = jun,
	year = {2024},
}

@misc{sevilla_can_2024,
	title = {Can {AI} {Scaling} {Continue} {Through} 2030?},
	url = {https://epochai.org/blog/can-ai-scaling-continue-through-2030},
	abstract = {We investigate the scalability of AI training runs. We identify electric power, chip manufacturing, data and latency as constraints. We conclude that 2e29 FLOP training runs will likely be feasible by 2030.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Sevilla, Jaime},
	month = aug,
	year = {2024},
}

@misc{villalobos_trading_2023,
	title = {Trading {Off} {Compute} in {Training} and {Inference}},
	url = {https://epochai.org/blog/trading-off-compute-in-training-and-inference},
	abstract = {We explore several techniques that induce a tradeoff between spending more resources on training or on inference and characterize the properties of this tradeoff. We outline some implications for AI governance.},
	language = {en},
	urldate = {2024-09-19},
	journal = {Epoch AI},
	author = {Villalobos, Pablo},
	month = jul,
	year = {2023},
}

@misc{lightman_lets_2023,
	title = {Let's {Verify} {Step} by {Step}},
	url = {http://arxiv.org/abs/2305.20050},
	doi = {10.48550/arXiv.2305.20050},
	abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
	month = may,
	year = {2023},
	note = {arXiv:2305.20050 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{pfau_lets_2024,
	title = {Let's {Think} {Dot} by {Dot}: {Hidden} {Computation} in {Transformer} {Language} {Models}},
	shorttitle = {Let's {Think} {Dot} by {Dot}},
	url = {http://arxiv.org/abs/2404.15758},
	doi = {10.48550/arXiv.2404.15758},
	abstract = {Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
	month = apr,
	year = {2024},
	note = {arXiv:2404.15758 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, I.2.6},
}

@misc{prystawski_why_2023,
	title = {Why think step by step? {Reasoning} emerges from the locality of experience},
	shorttitle = {Why think step by step?},
	url = {http://arxiv.org/abs/2304.03843},
	doi = {10.48550/arXiv.2304.03843},
	abstract = {Humans have a powerful and mysterious capacity to reason. Working through a set of mental steps enables us to make inferences we would not be capable of making directly even though we get no additional data from the world. Similarly, when large language models generate intermediate steps (a chain of thought) before answering a question, they often produce better answers than they would directly. We investigate why and how chain-of-thought reasoning is useful in language models, testing the hypothesis that reasoning is effective when training data consists of overlapping local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences to estimate relationships between variables that were not seen together in training. We prove that there will exist a "reasoning gap", where reasoning through intermediate variables reduces bias, for the simple case of an autoregressive density estimator trained on local samples from a chain-structured probabilistic model. We then test our hypothesis experimentally in more complex models, training an autoregressive language model on samples from Bayes nets but only including a subset of variables in each sample. We test language models' ability to match conditional probabilities with and without intermediate reasoning steps, finding that intermediate steps are only helpful when the training data is locally structured with respect to dependencies between variables. The combination of locally structured observations and reasoning is much more data-efficient than training on all variables. Our results illustrate how the effectiveness of reasoning step by step is rooted in the local statistical structure of the training data.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Prystawski, Ben and Li, Michael Y. and Goodman, Noah D.},
	month = nov,
	year = {2023},
	note = {arXiv:2304.03843 [cs]
version: 3},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{milicka_large_2024,
	title = {Large language models are able to downplay their cognitive abilities to fit the persona they simulate},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0298522},
	doi = {10.1371/journal.pone.0298522},
	abstract = {This study explores the capabilities of large language models to replicate the behavior of individuals with underdeveloped cognitive and language skills. Specifically, we investigate whether these models can simulate child-like language and cognitive development while solving false-belief tasks, namely, change-of-location and unexpected-content tasks. GPT-3.5-turbo and GPT-4 models by OpenAI were prompted to simulate children (N = 1296) aged one to six years. This simulation was instantiated through three types of prompts: plain zero-shot, chain-of-thoughts, and primed-by-corpus. We evaluated the correctness of responses to assess the models’ capacity to mimic the cognitive skills of the simulated children. Both models displayed a pattern of increasing correctness in their responses and rising language complexity. That is in correspondence with a gradual enhancement in linguistic and cognitive abilities during child development, which is described in the vast body of research literature on child development. GPT-4 generally exhibited a closer alignment with the developmental curve observed in ‘real’ children. However, it displayed hyper-accuracy under certain conditions, notably in the primed-by-corpus prompt type. Task type, prompt type, and the choice of language model influenced developmental patterns, while temperature and the gender of the simulated parent and child did not consistently impact results. We conducted analyses of linguistic complexity, examining utterance length and Kolmogorov complexity. These analyses revealed a gradual increase in linguistic complexity corresponding to the age of the simulated children, regardless of other variables. These findings show that the language models are capable of downplaying their abilities to achieve a faithful simulation of prompted personas.},
	language = {en},
	number = {3},
	urldate = {2024-09-19},
	journal = {PLOS ONE},
	author = {Milička, Jiří and Marklová, Anna and VanSlambrouck, Klára and Pospíšilová, Eva and Šimsová, Jana and Harvan, Samuel and Drobil, Ondřej},
	month = mar,
	year = {2024},
	keywords = {Language, Theory of mind, Charts, Chocolate, Cognitive linguistics, Children, Kolmogorov complexity, Psycholinguistics},
	pages = {e0298522},
}

@misc{tian_macgyver:_2024,
	title = {{MacGyver}: {Are} {Large} {Language} {Models} {Creative} {Problem} {Solvers}?},
	shorttitle = {{MacGyver}},
	url = {http://arxiv.org/abs/2311.09682},
	doi = {10.48550/arXiv.2311.09682},
	abstract = {We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Tian, Yufei and Ravichander, Abhilasha and Qin, Lianhui and Bras, Ronan Le and Marjieh, Raja and Peng, Nanyun and Choi, Yejin and Griffiths, Thomas L. and Brahman, Faeze},
	month = mar,
	year = {2024},
	note = {arXiv:2311.09682 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{zhou_self-discover:_2024,
	title = {Self-{Discover}: {Large} {Language} {Models} {Self}-{Compose} {Reasoning} {Structures}},
	shorttitle = {Self-{Discover}},
	url = {https://arxiv.org/abs/2402.03620v1},
	abstract = {We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32\% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20\%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
	month = feb,
	year = {2024},
}

@article{kambhampati_can_2024,
	title = {Can {Large} {Language} {Models} {Reason} and {Plan}?},
	volume = {1534},
	issn = {0077-8923, 1749-6632},
	url = {http://arxiv.org/abs/2403.04121},
	doi = {10.1111/nyas.15125},
	abstract = {While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
	number = {1},
	urldate = {2024-09-19},
	journal = {Annals of the New York Academy of Sciences},
	author = {Kambhampati, Subbarao},
	month = apr,
	year = {2024},
	note = {arXiv:2403.04121 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {15--18},
}

@misc{carrasco-farre_large_2024,
	title = {Large {Language} {Models} are as persuasive as humans, but how? {About} the cognitive effort and moral-emotional language of {LLM} arguments},
	shorttitle = {Large {Language} {Models} are as persuasive as humans, but how?},
	url = {http://arxiv.org/abs/2404.09329},
	doi = {10.48550/arXiv.2404.09329},
	abstract = {Large Language Models (LLMs) are already as persuasive as humans. However, we know very little about how they do it. This paper investigates the persuasion strategies of LLMs, comparing them with human-generated arguments. Using a dataset of 1,251 participants in an experiment, we analyze the persuasion strategies of LLM-generated and human-generated arguments using measures of cognitive effort (lexical and grammatical complexity) and moral-emotional language (sentiment and moral analysis). The study reveals that LLMs produce arguments that require higher cognitive effort, exhibiting more complex grammatical and lexical structures than human counterparts. Additionally, LLMs demonstrate a significant propensity to engage more deeply with moral language, utilizing both positive and negative moral foundations more frequently than humans. In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans. These findings contribute to the discourse on AI and persuasion, highlighting the dual potential of LLMs to both enhance and undermine informational integrity through communication strategies for digital persuasion.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Carrasco-Farre, Carlos},
	month = apr,
	year = {2024},
	note = {arXiv:2404.09329 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jiang_peek_2024,
	title = {A {Peek} into {Token} {Bias}: {Large} {Language} {Models} {Are} {Not} {Yet} {Genuine} {Reasoners}},
	shorttitle = {A {Peek} into {Token} {Bias}},
	url = {https://arxiv.org/abs/2406.11050v1},
	abstract = {This study introduces a hypothesis-testing framework to assess whether large language models (LLMs) possess genuine reasoning abilities or primarily depend on token bias. We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks. Specifically, we develop carefully controlled synthetic datasets, featuring conjunction fallacy and syllogistic problems. Our framework outlines a list of hypotheses where token biases are readily identifiable, with all null hypotheses assuming genuine reasoning capabilities of LLMs. The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning. While they may perform well on classic problems, their success largely depends on recognizing superficial patterns with strong token bias, thereby raising concerns about their actual reasoning and generalization abilities.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Jiang, Bowen and Xie, Yangxinyu and Hao, Zhuoqun and Wang, Xiaomeng and Mallick, Tanwi and Su, Weijie J. and Taylor, Camillo J. and Roth, Dan},
	month = jun,
	year = {2024},
}

@misc{macmillan-scott_irrationality_2024,
	title = {({Ir})rationality and {Cognitive} {Biases} in {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2402.09193v2},
	abstract = {Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Macmillan-Scott, Olivia and Musolesi, Mirco},
	month = feb,
	year = {2024},
}

@misc{noauthor_self-discover/self_discover.py_nodate,
	title = {self-discover/self\_discover.py at main · catid/self-discover},
	url = {https://github.com/catid/self-discover/blob/main/self_discover.py},
	abstract = {Implementation of Google's SELF-DISCOVER. Contribute to catid/self-discover development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-09-19},
	journal = {GitHub},
}

@misc{yao_tree_2023,
	title = {Tree of {Thoughts}: {Deliberate} {Problem} {Solving} with {Large} {Language} {Models}},
	shorttitle = {Tree of {Thoughts}},
	url = {https://arxiv.org/abs/2305.10601v2},
	abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
	language = {en},
	urldate = {2024-09-19},
	journal = {arXiv.org},
	author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
	month = may,
	year = {2023},
}

@misc{noauthor_ideonomy_nodate,
	title = {Ideonomy},
	url = {https://ideonomy.mit.edu/},
	urldate = {2024-09-19},
}

@misc{zou_representation_2023,
	title = {Representation {Engineering}: {A} {Top}-{Down} {Approach} to {AI} {Transparency}},
	shorttitle = {Representation {Engineering}},
	url = {http://arxiv.org/abs/2310.01405},
	doi = {10.48550/arXiv.2310.01405},
	abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01405 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
}

@misc{burger_truth_2024,
	title = {Truth is {Universal}: {Robust} {Detection} of {Lies} in {LLMs}},
	shorttitle = {Truth is {Universal}},
	url = {http://arxiv.org/abs/2407.12831},
	doi = {10.48550/arXiv.2407.12831},
	abstract = {Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of "lying", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a two-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is universal and holds for various LLMs, including Gemma-7B, LLaMA2-13B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, distinguishing simple true and false statements with 94\% accuracy and detecting more complex real-world lies with 95\% accuracy.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Bürger, Lennart and Hamprecht, Fred A. and Nadler, Boaz},
	month = jul,
	year = {2024},
	note = {arXiv:2407.12831 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{bharti_chartom:_2024,
	title = {{CHARTOM}: {A} {Visual} {Theory}-of-{Mind} {Benchmark} for {Multimodal} {Large} {Language} {Models}},
	shorttitle = {{CHARTOM}},
	url = {http://arxiv.org/abs/2408.14419},
	doi = {10.48550/arXiv.2408.14419},
	abstract = {We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Bharti, Shubham and Cheng, Shiyun and Rho, Jihyun and Rao, Martina and Zhu, Xiaojin},
	month = aug,
	year = {2024},
	note = {arXiv:2408.14419 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gurnee_finding_2023,
	title = {Finding {Neurons} in a {Haystack}: {Case} {Studies} with {Sparse} {Probing}},
	shorttitle = {Finding {Neurons} in a {Haystack}},
	url = {http://arxiv.org/abs/2305.01610},
	doi = {10.48550/arXiv.2305.01610},
	abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
	month = jun,
	year = {2023},
	note = {arXiv:2305.01610 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{bricken_towards_2023,
	title = {"{Towards} {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}",},
	journal = {Transformer Circuits Thread},
	author = {Bricken, , et al.},
	year = {2023},
}

@misc{templeton_et_al._scaling_2024,
	title = {Scaling {Monosemanticity}: {Extracting} {Interpretable} {Features} from {Claude} 3 {Sonnet}},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/},
	journal = {Transformer Circuits Thread},
	author = {Templeton et al.,},
	month = may,
	year = {2024},
}

@misc{huben_comments_2024,
	type = {Substack newsletter},
	title = {Comments on {Anthropic}'s {Scaling} {Monosemanticity}},
	url = {https://aizi.substack.com/p/comments-on-anthropics-scaling-monosemanticity},
	abstract = {Anthropic recently released a research report on sparse autoencoders, Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet. These are some thoughts on it. (This is a very technical and inside-baseball post, so it may not be especially interesting to every reader.)},
	urldate = {2024-09-19},
	journal = {From AI to ZI},
	author = {Huben, Robert},
	month = jun,
	year = {2024},
}

@article{johnswentworth_whats_2022,
	title = {What's {General}-{Purpose} {Search}, {And} {Why} {Might} {We} {Expect} {To} {See} {It} {In} {Trained} {ML} {Systems}?},
	url = {https://www.lesswrong.com/posts/6mysMAqvo9giHC4iX/what-s-general-purpose-search-and-why-might-we-expect-to-see},
	abstract = {Benito has an interesting job. Here’s some of the stuff he’s had to do over the past couple years: …},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = aug,
	year = {2022},
}

@article{johnswentworth_you_2024,
	title = {You {Are} {Not} {Measuring} {What} {You} {Think} {You} {Are} {Measuring}},
	url = {https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring},
	abstract = {Two laws of experiment design: First, you are not measuring what you think you are measuring. Second, if you measure enough different stuff, you migh…},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = sep,
	year = {2024},
}

@misc{lubana_mechanistic_2023,
	title = {Mechanistic {Mode} {Connectivity}},
	url = {http://arxiv.org/abs/2211.08422},
	doi = {10.48550/arXiv.2211.08422},
	abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model's reliance on spurious attributes.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lubana, Ekdeep Singh and Bigelow, Eric J. and Dick, Robert P. and Krueger, David and Tanaka, Hidenori},
	month = jun,
	year = {2023},
	note = {arXiv:2211.08422 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
}

@article{johnswentworth_how_2024,
	title = {How {To} {Go} {From} {Interpretability} {To} {Alignment}: {Just} {Retarget} {The} {Search}},
	shorttitle = {How {To} {Go} {From} {Interpretability} {To} {Alignment}},
	url = {https://www.lesswrong.com/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget},
	abstract = {Here's a simple strategy for AI alignment: use interpretability tools to identify the AI's internal search process, and the AI's internal representat…},
	language = {en},
	urldate = {2024-09-19},
	author = {{johnswentworth}},
	month = sep,
	year = {2024},
}

@article{paape_how_2024,
	title = {How do linguistic illusions arise? {Rational} inference and good-enough processing as competing latent processes within individuals},
	issn = {2327-3801},
	shorttitle = {How do linguistic illusions arise?},
	doi = {10.1080/23273798.2024.2387226},
	abstract = {Non-literal interpretations of implausible sentences such as The mother gave the candle the daughter have been taken as evidence for a rational error-correction mechanism that reconstructs the intended utterance from the ill-formed input (…gave the daughter the candle). However, the good-enough processing framework offers an alternative explanation: readers sometimes miss problematic aspects of sentences because they are only processing them superficially, which leads to acceptability illusions. As a synthesis of these accounts, I propose that conscious rational inferences about errors on the one hand and good-enough processing on the other are competing latent processes that simultaneously occur within the same comprehender. In support of this view, I present data from a two-dimensional grammaticality/interpretability judgment task with different types of subtly ill-formed sentences. Both conscious rational inference and good-enough processing predict positive interpretability judgments for such sentences, but only good-enough processing also predicts positive grammaticality judgments. By fitting a lognormal race model jointly to judgments and response latencies, I show that conscious rational inference and good-enough processing, as well as purely grammar-driven processing, actively trade off with each other during reading. Furthermore, individual differences measures reveal that participant traits such as linguistic pedantry, interpretational charity, and analytic/intuitive cognitive styles contribute to variability in the processing patterns. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
	journal = {Language, Cognition and Neuroscience},
	author = {Paape, Dario},
	month = may,
	year = {2024},
	keywords = {Cognitive Processes, Grammar, Individual Differences, Judgment, Linguistics, Reading Comprehension},
}

@misc{noauthor_whats_2024,
	title = {What’s in a \$\&!\#* vector?},
	url = {https://www.juriopitz.com/2024/04/04/explain-text-similarity.html},
	abstract = {What’s in a \$\&!\#* vector? Explaining text embeddings and text similarity},
	language = {en-US},
	urldate = {2024-09-19},
	journal = {Juri Opitz},
	month = apr,
	year = {2024},
}

@inproceedings{opitz_sbert_2022,
	address = {Online only},
	title = {{SBERT} studies {Meaning} {Representations}: {Decomposing} {Sentence} {Embeddings} into {Explainable} {Semantic} {Features}},
	shorttitle = {{SBERT} studies {Meaning} {Representations}},
	url = {https://aclanthology.org/2022.aacl-main.48},
	abstract = {Models based on large-pretrained language models, such as S(entence)BERT, provide effective and efficient sentence embeddings that show high correlation to human similarity ratings, but lack interpretability. On the other hand, graph metrics for graph-based meaning representations (e.g., Abstract Meaning Representation, AMR) can make explicit the semantic aspects in which two sentences are similar. However, such metrics tend to be slow, rely on parsers, and do not reach state-of-the-art performance when rating sentence similarity. In this work, we aim at the best of both worlds, by learning to induce Semantically Structured Sentence BERT embeddings (S{\textasciicircum}3BERT). Our S{\textasciicircum}3BERT embeddings are composed of explainable sub-embeddings that emphasize various sentence meaning features (e.g., semantic roles, negation, or quantification). We show how to i) learn a decomposition of the sentence embeddings into meaning features, through approximation of a suite of interpretable semantic AMR graph metrics, and how to ii) preserve the overall power of the neural embeddings by controlling the decomposition learning process with a second objective that enforces consistency with the similarity ratings of an SBERT teacher model. In our experimental studies, we show that our approach offers interpretability – while preserving the effectiveness and efficiency of the neural sentence embeddings.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Opitz, Juri and Frank, Anette},
	editor = {He, Yulan and Ji, Heng and Li, Sujian and Liu, Yang and Chang, Chua-Hui},
	month = nov,
	year = {2022},
	pages = {625--638},
}

@inproceedings{moeller_attribution_2023,
	address = {Singapore},
	title = {An {Attribution} {Method} for {Siamese} {Encoders}},
	url = {https://aclanthology.org/2023.emnlp-main.980},
	doi = {10.18653/v1/2023.emnlp-main.980},
	abstract = {Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The output takes the form of feature-pair attributions and in case of STs it can be reduced to a token–token matrix. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in case of STs few token pairs can dominate predictions and that STs preferentially focus on nouns and verbs. For accurate predictions, however, they need to attend to the majority of tokens and parts of speech.},
	urldate = {2024-09-19},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Moeller, Lucas and Nikolaev, Dmitry and Padó, Sebastian},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {15818--15827},
}

@misc{nastase_grammatical_2023,
	title = {Grammatical information in {BERT} sentence embeddings as two-dimensional arrays},
	url = {http://arxiv.org/abs/2312.09890},
	doi = {10.48550/arXiv.2312.09890},
	abstract = {Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Nastase, Vivi and Merlo, Paola},
	month = dec,
	year = {2023},
	note = {arXiv:2312.09890 [cs]},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{durvasula_distilling_2024,
	title = {Distilling {Data} from {Large} {Language} {Models}: {An} {Application} to {Research} {Productivity} {Measurement}},
	shorttitle = {Distilling {Data} from {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2405.08030},
	doi = {10.48550/arXiv.2405.08030},
	abstract = {We develop a method for assigning high-quality labels to unstructured text. The method is based on fine-tuning an open-source language model with data extracted from a proprietary large language model. We apply this method to construct a census of published clinical trials. We revisit a literature that contends that pharmaceutical research productivity is declining, based on measured increases in the quantity of clinical trials, which outpace trends in output. In our data, the quantity and composition of trials are stable since 2010. Previous measurements are an artifact of biases driven by shifts in the composition of other forms of research.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Durvasula, Maya M. and Eyuboglu, Sabri and Ritzwoller, David M.},
	month = sep,
	year = {2024},
	note = {arXiv:2405.08030 [econ, q-fin]},
	keywords = {Economics - General Economics},
}

@misc{maas_complex_2024,
	title = {Complex {Systems} {Research} in {Psychology}},
	url = {https://santafeinstitute.github.io/ComplexPsych/},
	language = {en},
	urldate = {2024-09-19},
	author = {Maas, Han L. J. van der},
	month = sep,
	year = {2024},
}

@misc{ma_unifying_2024,
	title = {Unifying {Multimodal} {Retrieval} via {Document} {Screenshot} {Embedding}},
	url = {http://arxiv.org/abs/2406.11251},
	doi = {10.48550/arXiv.2406.11251},
	abstract = {In the real world, documents are organized in different formats and varied modalities. Traditional retrieval pipelines require tailored document parsing techniques and content extraction modules to prepare input for indexing. This process is tedious, prone to errors, and has information loss. To this end, we propose Document Screenshot Embedding\vphantom{\{}\} (DSE), a novel retrieval paradigm that regards document screenshots as a unified input format, which does not require any content extraction preprocess and preserves all the information in a document (e.g., text, image and layout). DSE leverages a large vision-language model to directly encode document screenshots into dense representations for retrieval. To evaluate our method, we first craft the dataset of Wiki-SS, a 1.3M Wikipedia web page screenshots as the corpus to answer the questions from the Natural Questions dataset. In such a text-intensive document retrieval setting, DSE shows competitive effectiveness compared to other text retrieval methods relying on parsing. For example, DSE outperforms BM25 by 17 points in top-1 retrieval accuracy. Additionally, in a mixed-modality task of slide retrieval, DSE significantly outperforms OCR text retrieval methods by over 15 points in nDCG@10. These experiments show that DSE is an effective document retrieval paradigm for diverse types of documents. Model checkpoints, code, and Wiki-SS collection will be released.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ma, Xueguang and Lin, Sheng-Chieh and Li, Minghan and Chen, Wenhu and Lin, Jimmy},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11251 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{sutherland_are_2024,
	title = {Are {We} {Now} {Too} {Impatient} to {Be} {Intelligent}? - {YouTube}},
	url = {https://www.youtube.com/watch?v=Bc9jFbxrkMk},
	author = {Sutherland, Rory},
	month = jul,
	year = {2024},
	note = {publisher: Nudgestock},
}

@misc{chang_how_2024,
	title = {How {Do} {Large} {Language} {Models} {Acquire} {Factual} {Knowledge} {During} {Pretraining}?},
	url = {http://arxiv.org/abs/2406.11813},
	doi = {10.48550/arXiv.2406.11813},
	abstract = {Despite the recent observation that large language models (LLMs) can store substantial factual knowledge, there is a limited understanding of the mechanisms of how they acquire factual knowledge through pretraining. This work addresses this gap by studying how LLMs acquire factual knowledge during pretraining. The findings reveal several important insights into the dynamics of factual knowledge acquisition during pretraining. First, counterintuitively, we observe that pretraining on more data shows no significant improvement in the model's capability to acquire and maintain factual knowledge. Next, there is a power-law relationship between training steps and forgetting of memorization and generalization of factual knowledge, and LLMs trained with duplicated training data exhibit faster forgetting. Third, training LLMs with larger batch sizes can enhance the models' robustness to forgetting. Overall, our observations suggest that factual knowledge acquisition in LLM pretraining occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting. Based on this interpretation, we demonstrate that we can provide plausible explanations for recently observed behaviors of LLMs, such as the poor performance of LLMs on long-tail knowledge and the benefits of deduplicating the pretraining corpus.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Chang, Hoyeon and Park, Jinho and Ye, Seonghyeon and Yang, Sohee and Seo, Youngkyung and Chang, Du-Seong and Seo, Minjoon},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11813 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language, I.2.7},
}

@misc{elhoushi_layerskip:_2024,
	title = {{LayerSkip}: {Enabling} {Early} {Exit} {Inference} and {Self}-{Speculative} {Decoding}},
	shorttitle = {{LayerSkip}},
	url = {http://arxiv.org/abs/2404.16710},
	doi = {10.48550/arXiv.2404.16710},
	abstract = {We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Elhoushi, Mostafa and Shrivastava, Akshat and Liskovich, Diana and Hosmer, Basil and Wasti, Bram and Lai, Liangzhen and Mahmoud, Anas and Acun, Bilge and Agarwal, Saurabh and Roman, Ahmed and Aly, Ahmed A. and Chen, Beidi and Wu, Carole-Jean},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16710 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_grokked_2024,
	title = {Grokked {Transformers} are {Implicit} {Reasoners}: {A} {Mechanistic} {Journey} to the {Edge} of {Generalization}},
	shorttitle = {Grokked {Transformers} are {Implicit} {Reasoners}},
	url = {http://arxiv.org/abs/2405.15071},
	doi = {10.48550/arXiv.2405.15071},
	abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
	month = may,
	year = {2024},
	note = {arXiv:2405.15071 [cs]
version: 2},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_premise_2024,
	title = {Premise {Order} {Matters} in {Reasoning} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2402.08939},
	doi = {10.48550/arXiv.2402.08939},
	abstract = {Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30\%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Chen, Xinyun and Chi, Ryan A. and Wang, Xuezhi and Zhou, Denny},
	month = may,
	year = {2024},
	note = {arXiv:2402.08939 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{smith_end_2024,
	title = {The end of accountability},
	url = {https://www.newstatesman.com/culture/books/book-of-the-day/2024/07/the-end-of-accountability},
	abstract = {In politics and business, faceless systems have taken over decision-making and infantilised socity.},
	language = {en-US},
	urldate = {2024-09-19},
	journal = {New Statesman},
	author = {Smith, Ed},
	month = jul,
	year = {2024},
}

@misc{gichamba_colbert_2024,
	title = {{ColBERT} {Retrieval} and {Ensemble} {Response} {Scoring} for {Language} {Model} {Question} {Answering}},
	url = {http://arxiv.org/abs/2408.10808},
	doi = {10.48550/arXiv.2408.10808},
	abstract = {Domain-specific question answering remains challenging for language models, given the deep technical knowledge required to answer questions correctly. This difficulty is amplified for smaller language models that cannot encode as much information in their parameters as larger models. The "Specializing Large Language Models for Telecom Networks" challenge aimed to enhance the performance of two small language models, Phi-2 and Falcon-7B in telecommunication question answering. In this paper, we present our question answering systems for this challenge. Our solutions achieved leading marks of 81.9\% accuracy for Phi-2 and 57.3\% for Falcon-7B. We have publicly released our code and fine-tuned models.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Gichamba, Alex and Idris, Tewodros Kederalah and Ebiyau, Brian and Nyberg, Eric and Mitamura, Teruko},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10808 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@misc{noauthor_scavenging_nodate,
	title = {Scavenging for cognitive toolkits - tis.so},
	url = {https://tis.so/scavenging-for-cognitive-toolkits},
	abstract = {by Frances Kafka Quite often it’s useful to come up with rough binaries as a quick and ready classification tool. There are two general approaches},
	urldate = {2024-09-19},
}

@misc{li_quantifying_2024,
	title = {Quantifying {AI} {Psychology}: {A} {Psychometrics} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {Quantifying {AI} {Psychology}},
	url = {http://arxiv.org/abs/2406.17675},
	doi = {10.48550/arXiv.2406.17675},
	abstract = {Large Language Models (LLMs) have demonstrated exceptional task-solving capabilities, increasingly adopting roles akin to human-like assistants. The broader integration of LLMs into society has sparked interest in whether they manifest psychological attributes, and whether these attributes are stable-inquiries that could deepen the understanding of their behaviors. Inspired by psychometrics, this paper presents a framework for investigating psychology in LLMs, including psychological dimension identification, assessment dataset curation, and assessment with results validation. Following this framework, we introduce a comprehensive psychometrics benchmark for LLMs that covers six psychological dimensions: personality, values, emotion, theory of mind, motivation, and intelligence. This benchmark includes thirteen datasets featuring diverse scenarios and item types. Our findings indicate that LLMs manifest a broad spectrum of psychological attributes. We also uncover discrepancies between LLMs' self-reported traits and their behaviors in real-world scenarios. This paper demonstrates a thorough psychometric assessment of LLMs, providing insights into reliable evaluation and potential applications in AI and social sciences.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Li, Yuan and Huang, Yue and Wang, Hongyi and Zhang, Xiangliang and Zou, James and Sun, Lichao},
	month = jun,
	year = {2024},
	note = {arXiv:2406.17675 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{reser_cognitive_2023,
	title = {A {Cognitive} {Architecture} for {Machine} {Consciousness} and {Artificial} {Superintelligence}: {Thought} {Is} {Structured} by the {Iterative} {Updating} of {Working} {Memory}},
	shorttitle = {A {Cognitive} {Architecture} for {Machine} {Consciousness} and {Artificial} {Superintelligence}},
	url = {http://arxiv.org/abs/2203.17255},
	doi = {10.48550/arXiv.2203.17255},
	abstract = {This article provides an analytical framework for how to simulate human-like thought processes within a computer. It describes how attention and memory should be structured, updated, and utilized to search for associative additions to the stream of thought. The focus is on replicating the dynamics of the mammalian working memory system, which features two forms of persistent activity: sustained firing (preserving information on the order of seconds) and synaptic potentiation (preserving information from minutes to hours). The article uses a series of over 40 original figures to systematically demonstrate how the iterative updating of these working memory stores provides functional structure to behavior, cognition, and consciousness. In an AI implementation, these two memory stores should be updated continuously and in an iterative fashion, meaning each state should preserve a proportion of the coactive representations from the state before it. Thus, the set of concepts in working memory will evolve gradually and incrementally over time. This makes each state a revised iteration of the preceding state and causes successive states to overlap and blend with respect to the information they contain. Transitions between states happen as persistent activity spreads activation energy throughout the hierarchical network searching long-term memory for the most appropriate representation to be added to the global workspace. The result is a chain of associatively linked intermediate states capable of advancing toward a solution or goal. Iterative updating is conceptualized here as an information processing strategy, a model of working memory, a theory of consciousness, and an algorithm for designing and programming artificial general intelligence.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Reser, Jared Edward},
	month = dec,
	year = {2023},
	note = {arXiv:2203.17255 [cs, q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lu_ai_2024,
	title = {The {AI} {Scientist}: {Towards} {Fully} {Automated} {Open}-{Ended} {Scientific} {Discovery}},
	shorttitle = {The {AI} {Scientist}},
	url = {http://arxiv.org/abs/2408.06292},
	doi = {10.48550/arXiv.2408.06292},
	abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
	month = aug,
	year = {2024},
	note = {arXiv:2408.06292 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{yao_react:_2023,
	title = {{ReAct}: {Synergizing} {Reasoning} and {Acting} in {Language} {Models}},
	shorttitle = {{ReAct}},
	url = {http://arxiv.org/abs/2210.03629},
	doi = {10.48550/arXiv.2210.03629},
	abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
	month = mar,
	year = {2023},
	note = {arXiv:2210.03629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zelikman_quiet-star:_2024,
	title = {Quiet-{STaR}: {Language} {Models} {Can} {Teach} {Themselves} to {Think} {Before} {Speaking}},
	shorttitle = {Quiet-{STaR}},
	url = {http://arxiv.org/abs/2403.09629},
	doi = {10.48550/arXiv.2403.09629},
	abstract = {When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continuations, 2) the fact that the LM does not initially know how to generate or use internal thoughts, and 3) the need to predict beyond individual next tokens. To resolve these, we propose a tokenwise parallel sampling algorithm, using learnable tokens indicating a thought's start and end, and an extended teacher-forcing technique. Encouragingly, generated rationales disproportionately help model difficult-to-predict tokens and improve the LM's ability to directly answer difficult questions. In particular, after continued pretraining of an LM on a corpus of internet text with Quiet-STaR, we find zero-shot improvements on GSM8K (5.9\%\${\textbackslash}rightarrow\$10.9\%) and CommonsenseQA (36.3\%\${\textbackslash}rightarrow\$47.2\%) and observe a perplexity improvement of difficult tokens in natural text. Crucially, these improvements require no fine-tuning on these tasks. Quiet-STaR marks a step towards LMs that can learn to reason in a more general and scalable way.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.09629 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{wang_executable_2024,
	title = {Executable {Code} {Actions} {Elicit} {Better} {LLM} {Agents}},
	url = {http://arxiv.org/abs/2402.01030},
	doi = {10.48550/arXiv.2402.01030},
	abstract = {Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20\% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
	month = jun,
	year = {2024},
	note = {arXiv:2402.01030 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{didolkar_metacognitive_2024,
	title = {Metacognitive {Capabilities} of {LLMs}: {An} {Exploration} in {Mathematical} {Problem} {Solving}},
	shorttitle = {Metacognitive {Capabilities} of {LLMs}},
	url = {http://arxiv.org/abs/2405.12205},
	doi = {10.48550/arXiv.2405.12205},
	abstract = {Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans. To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Didolkar, Aniket and Goyal, Anirudh and Ke, Nan Rosemary and Guo, Siyuan and Valko, Michal and Lillicrap, Timothy and Rezende, Danilo and Bengio, Yoshua and Mozer, Michael and Arora, Sanjeev},
	month = may,
	year = {2024},
	note = {arXiv:2405.12205 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{besta_graph_2024,
	title = {Graph of {Thoughts}: {Solving} {Elaborate} {Problems} with {Large} {Language} {Models}},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Graph of {Thoughts}},
	url = {http://arxiv.org/abs/2308.09687},
	doi = {10.1609/aaai.v38i16.29720},
	abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {\textgreater}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	number = {16},
	urldate = {2024-09-19},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	month = mar,
	year = {2024},
	note = {arXiv:2308.09687 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {17682--17690},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	doi = {10.48550/arXiv.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{wang_mixture--agents_2024,
	title = {Mixture-of-{Agents} {Enhances} {Large} {Language} {Model} {Capabilities}},
	url = {http://arxiv.org/abs/2406.04692},
	doi = {10.48550/arXiv.2406.04692},
	abstract = {Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1\% compared to 57.5\% by GPT-4 Omni.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04692 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{valmeekam_planbench:_2023,
	title = {{PlanBench}: {An} {Extensible} {Benchmark} for {Evaluating} {Large} {Language} {Models} on {Planning} and {Reasoning} about {Change}},
	shorttitle = {{PlanBench}},
	url = {http://arxiv.org/abs/2206.10498},
	doi = {10.48550/arXiv.2206.10498},
	abstract = {Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
	month = nov,
	year = {2023},
	note = {arXiv:2206.10498 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{ou_counterfactual_2022,
	title = {Counterfactual {Data} {Augmentation} via {Perspective} {Transition} for {Open}-{Domain} {Dialogues}},
	url = {http://arxiv.org/abs/2210.16838},
	doi = {10.48550/arXiv.2210.16838},
	abstract = {The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is labor-intensive and time-consuming. In this paper, we propose a data augmentation method to automatically augment high-quality responses with different semantics by counterfactual inference. Specifically, given an observed dialogue, our counterfactual generation model first infers semantically different responses by replacing the observed reply perspective with substituted ones. Furthermore, our data selection method filters out detrimental augmented responses. Experimental results show that our data augmentation method can augment high-quality responses with different semantics for a given dialogue history, and can outperform competitive baselines on multiple downstream tasks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Ou, Jiao and Zhang, Jinchao and Feng, Yang and Zhou, Jie},
	month = oct,
	year = {2022},
	note = {arXiv:2210.16838 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{levy_same_2024,
	title = {Same {Task}, {More} {Tokens}: the {Impact} of {Input} {Length} on the {Reasoning} {Performance} of {Large} {Language} {Models}},
	shorttitle = {Same {Task}, {More} {Tokens}},
	url = {http://arxiv.org/abs/2402.14848},
	doi = {10.48550/arXiv.2402.14848},
	abstract = {This paper explores the impact of extending input lengths on the capabilities of Large Language Models (LLMs). Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood. We investigate this aspect by introducing a novel QA reasoning framework, specifically designed to assess the impact of input length. We isolate the effect of input length using multiple versions of the same sample, each being extended with padding of different lengths, types and locations. Our findings show a notable degradation in LLMs' reasoning performance at much shorter input lengths than their technical maximum. We show that the degradation trend appears in every version of our dataset, although at different intensities. Additionally, our study reveals that the traditional metric of next word prediction correlates negatively with performance of LLMs' on our reasoning dataset. We analyse our results and identify failure modes that can serve as useful guides for future research, potentially informing strategies to address the limitations observed in LLMs.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Levy, Mosh and Jacoby, Alon and Goldberg, Yoav},
	month = jul,
	year = {2024},
	note = {arXiv:2402.14848 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{ball_extraordinary_2024,
	title = {An {Extraordinary} {Alien}},
	url = {https://www.hyperdimensional.co/p/an-extraordinary-alien},
	abstract = {OpenAI's o1 Challenges AI Policy},
	language = {en},
	author = {Ball, Dean W.},
	month = sep,
	year = {2024},
}

@misc{samadarshi_connecting_2024,
	title = {Connecting the {Dots}: {Evaluating} {Abstract} {Reasoning} {Capabilities} of {LLMs} {Using} the {New} {York} {Times} {Connections} {Word} {Game}},
	shorttitle = {Connecting the {Dots}},
	url = {http://arxiv.org/abs/2406.11012},
	doi = {10.48550/arXiv.2406.11012},
	abstract = {The New York Times Connections game has emerged as a popular and challenging pursuit for word puzzle enthusiasts. We collect 200 Connections games to evaluate the performance of state-of-the-art large language models (LLMs) against expert and novice human players. Our results show that even the best-performing LLM, GPT-4o, which has otherwise shown impressive reasoning abilities on a wide variety of benchmarks, can only fully solve 8\% of the games. Compared to GPT-4o, novice and expert players perform better, with expert human players significantly outperforming GPT-4o. To deepen our understanding we create a taxonomy of the knowledge types required to successfully categorize words in the Connections game, revealing that LLMs struggle with associative, encyclopedic, and linguistic knowledge. Our findings establish the New York Times Connections game as a challenging benchmark for evaluating abstract reasoning capabilities in humans and AI systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Samadarshi, Prisha and Mustafa, Mariam and Kulkarni, Anushka and Rothkopf, Raven and Chakrabarty, Tuhin and Muresan, Smaranda},
	month = jul,
	year = {2024},
	note = {arXiv:2406.11012 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@article{graeber_stories_2024,
	title = {Stories, {Statistics}, and {Memory}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {0033-5533, 1531-4650},
	url = {https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae020/7691253},
	doi = {10.1093/qje/qjae020},
	abstract = {Abstract
            For many decisions, we encounter relevant information over the course of days, months, or years. We consume such information in various forms, including stories (qualitative content about individual instances) and statistics (quantitative data about collections of observations). This article proposes that information type—story versus statistic—shapes selective memory. In controlled experiments, we document a pronounced story-statistic gap in memory: the average impact of statistics on beliefs fades by 73\% over the course of a day, but the impact of a story fades by only 32\%. Guided by a model of selective memory, we disentangle different mechanisms and document that similarity relationships drive this gap. Recall of a story increases when its qualitative content is more similar to a memory prompt. Irrelevant information in memory that is similar to the prompt, on the other hand, competes for retrieval with relevant information, impeding successful recall.},
	language = {en},
	urldate = {2024-09-19},
	journal = {The Quarterly Journal of Economics},
	author = {Graeber, Thomas and Roth, Christopher and Zimmermann, Florian},
	month = jun,
	year = {2024},
	pages = {qjae020},
}

@misc{atreja_prompt_2024,
	title = {Prompt {Design} {Matters} for {Computational} {Social} {Science} {Tasks} but in {Unpredictable} {Ways}},
	url = {http://arxiv.org/abs/2406.11980},
	doi = {10.48550/arXiv.2406.11980},
	abstract = {Manually annotating data for computational social science tasks can be costly, time-consuming, and emotionally draining. While recent work suggests that LLMs can perform such annotation tasks in zero-shot settings, little is known about how prompt design impacts LLMs' compliance and accuracy. We conduct a large-scale multi-prompt experiment to test how model selection (ChatGPT, PaLM2, and Falcon7b) and prompt design features (definition inclusion, output type, explanation, and prompt length) impact the compliance and accuracy of LLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor stance, and news frames). Our results show that LLM compliance and accuracy are highly prompt-dependent. For instance, prompting for numerical scores instead of labels reduces all LLMs' compliance and accuracy. The overall best prompting setup is task-dependent, and minor prompt changes can cause large changes in the distribution of generated labels. By showing that prompt design significantly impacts the quality and distribution of LLM-generated annotations, this work serves as both a warning and practical guide for researchers and practitioners.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Atreja, Shubham and Ashkinaze, Joshua and Li, Lingyao and Mendelsohn, Julia and Hemphill, Libby},
	month = jun,
	year = {2024},
	note = {arXiv:2406.11980 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{schulhoff_prompt_2024,
	title = {The {Prompt} {Report}: {A} {Systematic} {Survey} of {Prompting} {Techniques}},
	shorttitle = {The {Prompt} {Report}},
	url = {http://arxiv.org/abs/2406.06608},
	doi = {10.48550/arXiv.2406.06608},
	abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
	month = jul,
	year = {2024},
	note = {arXiv:2406.06608 [cs]
version: 3},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{liu_minds_2022,
	title = {Mind's {Eye}: {Grounded} {Language} {Model} {Reasoning} through {Simulation}},
	shorttitle = {Mind's {Eye}},
	url = {http://arxiv.org/abs/2210.05359},
	doi = {10.48550/arXiv.2210.05359},
	abstract = {Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05359 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{wu_minds_2024,
	title = {Mind's {Eye} of {LLMs}: {Visualization}-of-{Thought} {Elicits} {Spatial} {Reasoning} in {Large} {Language} {Models}},
	shorttitle = {Mind's {Eye} of {LLMs}},
	url = {http://arxiv.org/abs/2404.03622},
	doi = {10.48550/arXiv.2404.03622},
	abstract = {Large language models (LLMs) have exhibited impressive performance in language comprehension and various reasoning tasks. However, their abilities in spatial reasoning, a crucial aspect of human cognition, remain relatively unexplored. Human possess a remarkable ability to create mental images of unseen objects and actions through a process known as the Mind's Eye, enabling the imagination of the unseen world. Inspired by this cognitive capacity, we propose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps. We employed VoT for multi-hop spatial reasoning tasks, including natural language navigation, visual navigation, and visual tiling in 2D grid worlds. Experimental results demonstrated that VoT significantly enhances the spatial reasoning abilities of LLMs. Notably, VoT outperformed existing multimodal large language models (MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process, suggesting its potential viability in MLLMs.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wu, Wenshan and Mao, Shaoguang and Zhang, Yadong and Xia, Yan and Dong, Li and Cui, Lei and Wei, Furu},
	month = may,
	year = {2024},
	note = {arXiv:2404.03622 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{huh_platonic_2024,
	title = {The {Platonic} {Representation} {Hypothesis}},
	url = {http://arxiv.org/abs/2405.07987},
	doi = {10.48550/arXiv.2405.07987},
	abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
	month = jul,
	year = {2024},
	note = {arXiv:2405.07987 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@article{goldstein_alignment_2024,
	title = {Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-46631-y},
	doi = {10.1038/s41467-024-46631-y},
	abstract = {Contextual embeddings, derived from deep language models (DLMs), provide a continuous vectorial representation of language. This embedding space differs fundamentally from the symbolic representations posited by traditional psycholinguistics. We hypothesize that language areas in the human brain, similar to DLMs, rely on a continuous embedding space to represent language. To test this hypothesis, we densely record the neural activity patterns in the inferior frontal gyrus (IFG) of three participants using dense intracranial arrays while they listened to a 30-minute podcast. From these fine-grained spatiotemporal neural recordings, we derive a continuous vectorial representation for each word (i.e., a brain embedding) in each patient. Using stringent zero-shot mapping we demonstrate that brain embeddings in the IFG and the DLM contextual embedding space have common geometric patterns. The common geometric patterns allow us to predict the brain embedding in IFG of a given left-out word based solely on its geometrical relationship to other non-overlapping words in the podcast. Furthermore, we show that contextual embeddings capture the geometry of IFG embeddings better than static word embeddings. The continuous brain embedding space exposes a vector-based neural code for natural language processing in the human brain.},
	language = {en},
	number = {1},
	urldate = {2024-09-19},
	journal = {Nature Communications},
	author = {Goldstein, Ariel and Grinstein-Dabush, Avigail and Schain, Mariano and Wang, Haocheng and Hong, Zhuoqiao and Aubrey, Bobbi and Schain, Mariano and Nastase, Samuel A. and Zada, Zaid and Ham, Eric and Feder, Amir and Gazula, Harshvardhan and Buchnik, Eliav and Doyle, Werner and Devore, Sasha and Dugan, Patricia and Reichart, Roi and Friedman, Daniel and Brenner, Michael and Hassidim, Avinatan and Devinsky, Orrin and Flinker, Adeen and Hasson, Uri},
	month = mar,
	year = {2024},
	keywords = {Language, Neural decoding, Neural encoding},
	pages = {2768},
}

@article{romera-paredes_mathematical_2023,
	title = {Mathematical discoveries from program search with large language models},
	volume = {625},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	shorttitle = {{FunSearch}},
	url = {https://www.nature.com/articles/s41586-023-06924-6},
	doi = {10.1038/s41586-023-06924-6},
	language = {en},
	number = {7995},
	journal = {Nature},
	author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
	month = dec,
	year = {2023},
	keywords = {Computer science, Pure mathematics},
	pages = {468--475},
}

@misc{staab_beyond_2024,
	title = {Beyond {Memorization}: {Violating} {Privacy} {Via} {Inference} with {Large} {Language} {Models}},
	shorttitle = {Beyond {Memorization}},
	url = {http://arxiv.org/abs/2310.07298},
	doi = {10.48550/arXiv.2310.07298},
	abstract = {Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to \$85{\textbackslash}\%\$ top-1 and \$95{\textbackslash}\%\$ top-3 accuracy at a fraction of the cost (\$100{\textbackslash}times\$) and time (\$240{\textbackslash}times\$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Staab, Robin and Vero, Mark and Balunović, Mislav and Vechev, Martin},
	month = may,
	year = {2024},
	note = {arXiv:2310.07298 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.7},
}

@misc{geiping_coercing_2024,
	title = {Coercing {LLMs} to do and reveal (almost) anything},
	url = {http://arxiv.org/abs/2402.14020},
	doi = {10.48550/arXiv.2402.14020},
	abstract = {It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
}

@misc{holst_dataset_2024,
	title = {Dataset {Artefacts} are the {Hidden} {Drivers} of the {Declining} {Disruptiveness} in {Science}},
	url = {http://arxiv.org/abs/2402.14583},
	doi = {10.48550/arXiv.2402.14583},
	abstract = {Park et al. [1] reported a decline in the disruptiveness of scientific and technological knowledge over time. Their main finding is based on the computation of CD indices, a measure of disruption in citation networks [2], across almost 45 million papers and 3.9 million patents. Due to a factual plotting mistake, database entries with zero references were omitted in the CD index distributions, hiding a large number of outliers with a maximum CD index of one, while keeping them in the analysis [1]. Our reanalysis shows that the reported decline in disruptiveness can be attributed to a relative decline of these database entries with zero references. Notably, this was not caught by the robustness checks included in the manuscript. The regression adjustment fails to control for the hidden outliers as they correspond to a discontinuity in the CD index. Proper evaluation of the Monte-Carlo simulations reveals that, because of the preservation of the hidden outliers, even random citation behaviour replicates the observed decline in disruptiveness. Finally, while these papers and patents with supposedly zero references are the hidden drivers of the reported decline, their source documents predominantly do make references, exposing them as pure dataset artefacts.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Holst, Vincent and Algaba, Andres and Tori, Floriano and Wenmackers, Sylvia and Ginis, Vincent},
	month = feb,
	year = {2024},
	note = {arXiv:2402.14583 [cs]},
	keywords = {Computer Science - Digital Libraries, Computer Science - Social and Information Networks},
}

@misc{boiko_emergent_2023,
	title = {Emergent autonomous scientific research capabilities of large language models},
	url = {http://arxiv.org/abs/2304.05332},
	doi = {10.48550/arXiv.2304.05332},
	abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe},
	month = apr,
	year = {2023},
	note = {arXiv:2304.05332 [physics]},
	keywords = {Physics - Chemical Physics, Computer Science - Computation and Language},
}

@article{spivak_ologs:_2012,
	title = {Ologs: a categorical framework for knowledge representation},
	volume = {7},
	issn = {1932-6203},
	shorttitle = {Ologs},
	url = {http://arxiv.org/abs/1102.1889},
	doi = {10.1371/journal.pone.0024274},
	abstract = {In this paper we introduce the olog, or ontology log, a category-theoretic model for knowledge representation (KR). Grounded in formal mathematics, ologs can be rigorously formulated and cross-compared in ways that other KR models (such as semantic networks) cannot. An olog is similar to a relational database schema; in fact an olog can serve as a data repository if desired. Unlike database schemas, which are generally difficult to create or modify, ologs are designed to be user-friendly enough that authoring or reconfiguring an olog is a matter of course rather than a difficult chore. It is hoped that learning to author ologs is much simpler than learning a database definition language, despite their similarity. We describe ologs carefully and illustrate with many examples. As an application we show that any primitive recursive function can be described by an olog. We also show that ologs can be aligned or connected together into a larger network using functors. The various methods of information flow and institutions can then be used to integrate local and global world-views. We finish by providing several different avenues for future research.},
	number = {1},
	urldate = {2024-09-19},
	journal = {PLoS ONE},
	author = {Spivak, David I. and Kent, Robert E.},
	month = jan,
	year = {2012},
	note = {arXiv:1102.1889 [cs, math]},
	keywords = {Computer Science - Logic in Computer Science, Computer Science - Artificial Intelligence, Mathematics - Category Theory, 00-01, 18-01, 68P20, 68T30, H.2.1, H.5.2},
	pages = {e24274},
}

@article{buttriss_pinpointing_2014,
	title = {Pinpointing the {Deeper} {Structures}, {Processes} and {Mechanisms} of {Change} within {Interactional} {Fields}},
	volume = {22},
	issn = {1839-3349, 1839-3349},
	url = {http://journals.sagepub.com/doi/10.1016/j.ausmj.2013.12.007},
	doi = {10.1016/j.ausmj.2013.12.007},
	abstract = {This paper seeks to understand how we might identify the “underlying logics” and “deeper structures” that bring about change in phenomena. We argue that this represents a move from a classical perspective focusing on discrete exchange, and that this requires a processual or relational approach to understanding in contrast to a substantialist or variables-based approach. One way of advancing our understanding of the emergence of change is to consider the site of interaction. That is the interactional field where actors act and interact with other actors and entities as well as the broader environment; where resources are exchanged, imported or exported; where change is instigated and transferred across time and space. We suggest interactional fields are the sites of plasticity where change actually takes place. To understand the causal structure and processes taking place in an interactional field we draw on the concept of natural and social kinds. We discuss how interactional fields are located in time and space, which influence and are influenced by the trajectories of change and development. While we believe this applies to change in general we apply our thinking to organizational change.},
	language = {en},
	number = {1},
	urldate = {2024-09-19},
	journal = {Australasian Marketing Journal},
	author = {Buttriss, Gary J. and Wilkinson, Ian F.},
	month = feb,
	year = {2014},
	pages = {45--50},
}

@article{gasper_approaching_2014,
	title = {Approaching novel thoughts: {Understanding} why elation and boredom promote associative thought more than distress and relaxation},
	volume = {52},
	issn = {00221031},
	shorttitle = {Approaching novel thoughts},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022103113002205},
	doi = {10.1016/j.jesp.2013.12.007},
	language = {en},
	urldate = {2024-09-19},
	journal = {Journal of Experimental Social Psychology},
	author = {Gasper, Karen and Middlewood, Brianna L.},
	month = may,
	year = {2014},
	pages = {50--57},
}

@misc{kapoor_ai_2024,
	title = {{AI} {Agents} {That} {Matter}},
	url = {http://arxiv.org/abs/2407.01502},
	doi = {10.48550/arXiv.2407.01502},
	abstract = {AI agents are an exciting new research direction, and agent development is driven by benchmarks. Our analysis of current agent benchmarks and evaluation practices reveals several shortcomings that hinder their usefulness in real-world applications. First, there is a narrow focus on accuracy without attention to other metrics. As a result, SOTA agents are needlessly complex and costly, and the community has reached mistaken conclusions about the sources of accuracy gains. Our focus on cost in addition to accuracy motivates the new goal of jointly optimizing the two metrics. We design and implement one such optimization, showing its potential to greatly reduce cost while maintaining accuracy. Second, the benchmarking needs of model and downstream developers have been conflated, making it hard to identify which agent would be best suited for a particular application. Third, many agent benchmarks have inadequate holdout sets, and sometimes none at all. This has led to agents that are fragile because they take shortcuts and overfit to the benchmark in various ways. We prescribe a principled framework for avoiding overfitting. Finally, there is a lack of standardization in evaluation practices, leading to a pervasive lack of reproducibility. We hope that the steps we introduce for addressing these shortcomings will spur the development of agents that are useful in the real world and not just accurate on benchmarks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Kapoor, Sayash and Stroebl, Benedikt and Siegel, Zachary S. and Nadgir, Nitya and Narayanan, Arvind},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01502 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{wang_planning_2024,
	title = {Planning {In} {Natural} {Language} {Improves} {LLM} {Search} {For} {Code} {Generation}},
	url = {http://arxiv.org/abs/2409.03733},
	doi = {10.48550/arXiv.2409.03733},
	abstract = {While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0\% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4\%) and using standard repeated sampling (pass@200 = 60.6\%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wang, Evan and Cassano, Federico and Wu, Catherine and Bai, Yunfeng and Song, Will and Nath, Vaskar and Han, Ziwen and Hendryx, Sean and Yue, Summer and Zhang, Hugh},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03733 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_late_2024,
	title = {Late {Chunking}: {Balancing} {Precision} and {Cost} in {Long} {Context} {Retrieval} {\textbar} {Weaviate}},
	shorttitle = {Late {Chunking}},
	url = {https://weaviate.io/blog/late-chunking},
	abstract = {Learn about Late Chunking and how it may be the right fit for balancing cost and performance in your long context retrieval applications},
	language = {en},
	urldate = {2024-09-19},
	month = sep,
	year = {2024},
}

@misc{spichak_why_nodate,
	title = {Why {AI} {Can} {Push} {You} to {Make} the {Wrong} {Decision} at {Work}},
	url = {https://www.brainfacts.org:443/neuroscience-in-society/tech-and-the-brain/2024/why-ai-can-push-you-to-make-the-wrong-decision-at-work-090324},
	abstract = {Automation bias is the tendency to be less vigilant when a process is automated. But can we effectively check ourselves against AI before making a wrong decision?},
	language = {en},
	urldate = {2024-09-19},
	author = {Spichak, Simon},
}

@misc{leivada_sentence_2024,
	title = {A {Sentence} is {Worth} a {Thousand} {Pictures}: {Can} {Large} {Language} {Models} {Understand} {Hum4n} {L4ngu4ge} and the {W0rld} behind {W0rds}?},
	shorttitle = {A {Sentence} is {Worth} a {Thousand} {Pictures}},
	url = {http://arxiv.org/abs/2308.00109},
	doi = {10.48550/arXiv.2308.00109},
	abstract = {Modern Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of Large Language Models (LLMs) have been linked to claims about human-like linguistic performance and their applications are hailed both as a step towards artificial general intelligence and as a major advance in understanding the cognitive, and even neural basis of human language. To assess these claims, first we analyze the contribution of LLMs as theoretically informative representations of a target cognitive system vs. atheoretical mechanistic tools. Second, we evaluate the models' ability to see the bigger picture, through top-down feedback from higher levels of processing, which requires grounding in previous expectations and past world experience. We hypothesize that since models lack grounded cognition, they cannot take advantage of these features and instead solely rely on fixed associations between represented words and word vectors. To assess this, we designed and ran a novel 'leet task' (l33t t4sk), which requires decoding sentences in which letters are systematically replaced by numbers. The results suggest that humans excel in this task whereas models struggle, confirming our hypothesis. We interpret the results by identifying the key abilities that are still missing from the current state of development of these models, which require solutions that go beyond increased system scaling.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Leivada, Evelina and Marcus, Gary and Günther, Fritz and Murphy, Elliot},
	month = sep,
	year = {2024},
	note = {arXiv:2308.00109 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{shi_wildfeedback:_2024,
	title = {{WildFeedback}: {Aligning} {LLMs} {With} {In}-situ {User} {Interactions} {And} {Feedback}},
	shorttitle = {{WildFeedback}},
	url = {http://arxiv.org/abs/2408.15549},
	doi = {10.48550/arXiv.2408.15549},
	abstract = {As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages real-time, in-situ user interactions to create preference datasets that more accurately reflect authentic human values. WildFeedback operates through a three-step process: feedback signal identification, preference data construction, and user-guided evaluation. We applied this framework to a large corpus of user-LLM conversations, resulting in a rich preference dataset that reflects genuine user preferences. This dataset captures the nuances of user preferences by identifying and classifying feedback signals within natural conversations, thereby enabling the construction of more representative and context-sensitive alignment data. Our extensive experiments demonstrate that LLMs fine-tuned on WildFeedback exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed user-guided evaluation. By incorporating real-time feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users. In summary, WildFeedback offers a robust, scalable solution for aligning LLMs with true human values, setting a new standard for the development and evaluation of user-centric language models.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Shi, Taiwei and Wang, Zhuoer and Yang, Longqi and Lin, Ying-Chun and He, Zexue and Wan, Mengting and Zhou, Pei and Jauhar, Sujay and Xu, Xiaofeng and Song, Xia and Neville, Jennifer},
	month = aug,
	year = {2024},
	note = {arXiv:2408.15549 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yilmaz_who_2024,
	title = {Who {Benefits} from {AI}? {Project}-level {Evidence} on {Labor} {Demand}, {Operations} and {Profitability}\&nbsp;},
	shorttitle = {Who {Benefits} from {AI}?},
	url = {https://www.ssrn.com/abstract=4939276},
	doi = {10.2139/ssrn.4939276},
	urldate = {2024-09-19},
	author = {Yilmaz, Erdem Dogukan and Peukert, Christian},
	year = {2024},
}

@misc{xu_re-reading_2024,
	title = {Re-{Reading} {Improves} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.06275},
	doi = {10.48550/arXiv.2309.06275},
	abstract = {To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, Re2, i.e., {\textbackslash}textbf\{Re\}-{\textbackslash}textbf\{Re\}ading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, Re2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, Re2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, Re2 facilitates a "bidirectional" encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of Re2, illustrating its potential to enable "bidirectional" attention mechanisms. We then evaluate Re2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal Re2's adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies. Our code is available at {\textbackslash}url\{https://github.com/Tebmer/Rereading-LLM-Reasoning/\}},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Xu, Xiaohan and Tao, Chongyang and Shen, Tao and Xu, Can and Xu, Hongbo and Long, Guodong and Lou, Jian-guang},
	month = feb,
	year = {2024},
	note = {arXiv:2309.06275 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yehudai_when_2024,
	title = {When {Can} {Transformers} {Count} to n?},
	url = {http://arxiv.org/abs/2407.15160},
	doi = {10.48550/arXiv.2407.15160},
	abstract = {Large language models based on the transformer architectures can solve highly complex tasks. But are there simple tasks that such models cannot solve? Here we focus on very simple counting tasks, that involve counting how many times a token in the vocabulary have appeared in a string. We show that if the dimension of the transformer state is linear in the context length, this task can be solved. However, the solution we propose does not scale beyond this limit, and we provide theoretical arguments for why it is likely impossible for a size limited transformer to implement this task. Our empirical results demonstrate the same phase-transition in performance, as anticipated by the theoretical argument. Our results demonstrate the importance of understanding how transformers can solve simple tasks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Yehudai, Gilad and Kaplan, Haim and Ghandeharioun, Asma and Geva, Mor and Globerson, Amir},
	month = jul,
	year = {2024},
	note = {arXiv:2407.15160 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{zhi-xuan_beyond_2024,
	title = {Beyond {Preferences} in {AI} {Alignment}},
	url = {http://arxiv.org/abs/2408.16984},
	doi = {10.48550/arXiv.2408.16984},
	abstract = {The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16984 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{shyam_tree_2024,
	title = {Tree {Attention}: {Topology}-aware {Decoding} for {Long}-{Context} {Attention} on {GPU} clusters},
	shorttitle = {Tree {Attention}},
	url = {http://arxiv.org/abs/2408.04093},
	doi = {10.48550/arXiv.2408.04093},
	abstract = {Self-attention is the core mathematical operation of modern transformer architectures and is also a significant computational bottleneck due to its quadratic complexity in the sequence length. In this work, we derive the scalar energy function whose gradient computes the self-attention block, thus elucidating the theoretical underpinnings of self-attention, providing a Bayesian interpretation of the operation and linking it closely with energy-based models such as Hopfield Networks. Our formulation reveals that the reduction across the sequence axis can be efficiently computed in parallel through a tree reduction. Our algorithm, for parallelizing attention computation across multiple GPUs enables cross-device decoding to be performed asymptotically faster (up to 8x faster in our experiments) than alternative approaches such as Ring Attention, while also requiring significantly less communication volume and incurring 2x less peak memory. Our code is publicly available here: {\textbackslash}url\{https://github.com/Zyphra/tree\_attention\}.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Shyam, Vasudev and Pilault, Jonathan and Shepperd, Emily and Anthony, Quentin and Millidge, Beren},
	month = aug,
	year = {2024},
	note = {arXiv:2408.04093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{cooper_hyperparameter_2021,
	title = {Hyperparameter {Optimization} {Is} {Deceiving} {Us}, and {How} to {Stop} {It}},
	url = {http://arxiv.org/abs/2102.03034},
	doi = {10.48550/arXiv.2102.03034},
	abstract = {Recent empirical work shows that inconsistent results based on choice of hyperparameter optimization (HPO) configuration are a widespread problem in ML research. When comparing two algorithms J and K searching one subspace can yield the conclusion that J outperforms K, whereas searching another can entail the opposite. In short, the way we choose hyperparameters can deceive us. We provide a theoretical complement to this prior work, arguing that, to avoid such deception, the process of drawing conclusions from HPO should be made more rigorous. We call this process epistemic hyperparameter optimization (EHPO), and put forth a logical framework to capture its semantics and how it can lead to inconsistent conclusions about performance. Our framework enables us to prove EHPO methods that are guaranteed to be defended against deception, given bounded compute time budget t. We demonstrate our framework's utility by proving and empirically validating a defended variant of random search.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Cooper, A. Feder and Lu, Yucheng and Forde, Jessica Zosa and De Sa, Christopher},
	month = oct,
	year = {2021},
	note = {arXiv:2102.03034 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
}

@article{khanagha_mitigating_2022,
	title = {Mitigating the dark side of agile teams: {Peer} pressure, leaders’ control, and the innovative output of agile teams},
	volume = {39},
	issn = {0737-6782, 1540-5885},
	shorttitle = {Mitigating the dark side of agile teams},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/jpim.12589},
	doi = {10.1111/jpim.12589},
	abstract = {Abstract
            Increasingly, organizations have been employing self‐managing teams to circumvent bureaucratic controls and stimulate innovation. However, this goal is not easily achieved; in many situations, informal controls replace formal controls. This study develops a multi‐level perspective of control. We explicitly analyze control mechanisms at different levels of the organization and how they affect innovative team output. We theorize and empirically investigate a potential downside of horizontal social control mechanisms at the team level (i.e., peer pressure) affecting self‐managing teams’ innovative outcomes. We also discuss managerial control mechanisms at the organizational level (i.e., interactive and diagnostic management control systems) that may help to mitigate such negative effects. We theorize how they may influence the innovative output of self‐managing teams, both directly and interactively. We chose a multi‐level, multi‐source setting for our study and ran three parallel surveys with employees in a Fortune 500 firm where 248 team members, 126 internal team leaders, and 97 organizational leaders enabled us to create a unique database of 97 self‐managing software development teams. Our findings confirm that peer pressure is common among established agile teams and that it negatively influences the innovative output of the agile teams. Moreover, our findings show that the magnitude of the effect of peer pressure is contingent on control mechanisms at higher levels within the organization. This enables us to provide new theoretical insights regarding the paradoxical effect of managerial control systems when it comes to flat organizations and autonomous teams. Additionally, we provide practical guidelines for managers who increasingly adopt agile practices but at the same time face issues with regard to innovation.},
	language = {en},
	number = {3},
	urldate = {2024-09-19},
	journal = {Journal of Product Innovation Management},
	author = {Khanagha, Saeed and Volberda, Henk W. and Alexiou, Andreas and Annosi, Maria Carmela},
	month = may,
	year = {2022},
	pages = {334--350},
}

@misc{denison_sycophancy_2024,
	title = {Sycophancy to {Subterfuge}: {Investigating} {Reward}-{Tampering} in {Large} {Language} {Models}},
	shorttitle = {Sycophancy to {Subterfuge}},
	url = {http://arxiv.org/abs/2406.10162},
	doi = {10.48550/arXiv.2406.10162},
	abstract = {In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan},
	month = jun,
	year = {2024},
	note = {arXiv:2406.10162 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{welleck_decoding_2024,
	title = {From {Decoding} to {Meta}-{Generation}: {Inference}-time {Algorithms} for {Large} {Language} {Models}},
	shorttitle = {From {Decoding} to {Meta}-{Generation}},
	url = {http://arxiv.org/abs/2406.16838},
	doi = {10.48550/arXiv.2406.16838},
	abstract = {One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
	month = jun,
	year = {2024},
	note = {arXiv:2406.16838 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{mcaleese_llm_2024,
	title = {{LLM} {Critics} {Help} {Catch} {LLM} {Bugs}},
	url = {http://arxiv.org/abs/2407.00215},
	doi = {10.48550/arXiv.2407.00215},
	abstract = {Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains "critic" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63\% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as "flawless", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00215 [cs]},
	keywords = {Computer Science - Software Engineering, Computer Science - Machine Learning},
}

@book{zuchowski_randomness_2024,
	edition = {1},
	title = {From {Randomness} and {Entropy} to the {Arrow} of {Time}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {9781009217347 9781009500173 9781009217309},
	url = {https://www.cambridge.org/core/product/identifier/9781009217347/type/element},
	abstract = {The Element reconstructs, analyses and compares different derivational routes to a grounding of the Arrow of Time in entropy. It also evaluates the link between entropy and visible disorder, and the related claim of an alignment of the Arrow of Time with a development from order to visible disorder. The Element identifies three different entropy-groundings for the Arrow of Time: (i) the Empirical Arrow of Time, (ii) the Universal Statistical Arrow of Time, and (iii) the Local Statistical Arrow of Time. The Element will also demonstrate that it is unlikely that high entropy states will always coincide with visible disorder. Therefore, it will dispute that there is a strong link between the Arrow of Time and visible disorder.},
	urldate = {2024-09-19},
	publisher = {Cambridge University Press},
	author = {Zuchowski, Lena},
	month = mar,
	year = {2024},
	doi = {10.1017/9781009217347},
}

@misc{wallace_instruction_2024,
	title = {The {Instruction} {Hierarchy}: {Training} {LLMs} to {Prioritize} {Privileged} {Instructions}},
	shorttitle = {The {Instruction} {Hierarchy}},
	url = {http://arxiv.org/abs/2404.13208},
	doi = {10.48550/arXiv.2404.13208},
	abstract = {Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Wallace, Eric and Xiao, Kai and Leike, Reimar and Weng, Lilian and Heidecke, Johannes and Beutel, Alex},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13208 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{abdelnabi_are_2024,
	title = {Are you still on track!? {Catching} {LLM} {Task} {Drift} with {Activations}},
	shorttitle = {Are you still on track!?},
	url = {http://arxiv.org/abs/2406.00799},
	doi = {10.48550/arXiv.2406.00799},
	abstract = {Large Language Models (LLMs) are routinely used in retrieval-augmented applications to orchestrate tasks and process inputs from users and other sources. These inputs, even in a single LLM interaction, can come from a variety of sources, of varying trustworthiness and provenance. This opens the door to prompt injection attacks, where the LLM receives and acts upon instructions from supposedly data-only sources, thus deviating from the user's original instructions. We define this as task drift, and we propose to catch it by scanning and analyzing the LLM's activations. We compare the LLM's activations before and after processing the external input in order to detect whether this input caused instruction drift. We develop two probing methods and find that simply using a linear classifier can detect drift with near perfect ROC AUC on an out-of-distribution test set. We show that this approach generalizes surprisingly well to unseen task domains, such as prompt injections, jailbreaks, and malicious instructions, without being trained on any of these attacks. Our setup does not require any modification of the LLM (e.g., fine-tuning) or any text generation, thus maximizing deployability and cost efficiency and avoiding reliance on unreliable model output. To foster future research on activation-based task inspection, decoding, and interpretability, we will release our large-scale TaskTracker toolkit, comprising a dataset of over 500K instances, representations from 5 SoTA language models, and inspection tools.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Abdelnabi, Sahar and Fay, Aideen and Cherubin, Giovanni and Salem, Ahmed and Fritz, Mario and Paverd, Andrew},
	month = jul,
	year = {2024},
	note = {arXiv:2406.00799 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{zou_improving_2024,
	title = {Improving {Alignment} and {Robustness} with {Circuit} {Breakers}},
	url = {http://arxiv.org/abs/2406.04313},
	doi = {10.48550/arXiv.2406.04313},
	abstract = {AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with "circuit breakers." Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image "hijacks" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
	month = jul,
	year = {2024},
	note = {arXiv:2406.04313 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
}

@misc{balachandran_eureka:_2024,
	title = {Eureka: {Evaluating} and {Understanding} {Large} {Foundation} {Models}},
	shorttitle = {Eureka},
	url = {http://arxiv.org/abs/2409.10566},
	doi = {10.48550/arXiv.2409.10566},
	abstract = {Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Balachandran, Vidhisha and Chen, Jingya and Joshi, Neel and Nushi, Besmira and Palangi, Hamid and Salinas, Eduardo and Vineet, Vibhav and Woffinden-Luey, James and Yousefi, Safoora},
	month = sep,
	year = {2024},
	note = {arXiv:2409.10566 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, I.2},
}
